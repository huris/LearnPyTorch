{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9bd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9660f7c",
   "metadata": {},
   "source": [
    "# PyTorch的数据读取机制\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/6.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de2474",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "创建可迭代的数据装载器，训练的时候，每一个for循环，每一次iteration，就是从DataLoader中获取一个batch_size大小的数据\n",
    "\n",
    "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
    "\n",
    "- dataset：Dataset类，决定数据从哪读取以及如何读取\n",
    "- batch_size：批大小\n",
    "- num_workers：多进程读取数\n",
    "- shuffle：每个epoch是否乱序\n",
    "- drop_last：样本数无法被batch_size整除时，是否舍弃最后一批数据\n",
    "\n",
    "训练概念：\n",
    "- epoch：所有训练样本都已输入到模型中，称为一个epoch\n",
    "- iteration：一批样本输入到模型中，称为一个iteration\n",
    "- batch_size：批大小，决定一个epoch有多少iteration\n",
    "\n",
    "假设样本总数80，batch_size为8，则1 epoch = 10 iteration。\n",
    "\n",
    "假设样本总数87，batch_size为8：\n",
    "- 如果drop_last=True，则1 epoch = 10 iteration\n",
    "- 如果drop_last=False，则 1 epoch = 11 iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521cadbf",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "torch.utils.data.Dataset()：Dataset抽象类，所有自定义的Dataset都需要继承它，并且必须复写\\_\\_getitem\\_\\_()类方法\n",
    "\n",
    "```\n",
    "class Dataset(object):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "\n",
    "\\_\\_getitem\\_\\_()：Dataset的核心，接收一个索引，返回一个样本。需要编写如何根据这个索引取读取数据部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad30997",
   "metadata": {},
   "source": [
    "## 数据读取机制实践\n",
    "\n",
    "1. 读取哪些数据？每一次迭代要读取batch_size大小的样本，那么读哪些样本呢？\n",
    "2. 从哪里读数据？在硬盘中该如何找数据，在哪设置参数？\n",
    "3. 怎么读数据？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f7fd6",
   "metadata": {},
   "source": [
    "### 人民币二分类\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/7.png\" width=\"70%\">\n",
    "\n",
    "1块的图片100张，100块的图片100张。\n",
    "\n",
    "任务是训练一个模型，对这两类图片进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fa9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMBDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        rmb面额分类任务的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        self.label_name = {\"1\": 0, \"100\": 1}\n",
    "        # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    # 拿到训练样本\n",
    "    def __getitem__(self, index):\n",
    "        path_img, label = self.data_info[index]  # 给定index获取样本\n",
    "        img = Image.open(path_img).convert('RGB')  # 找到图片转成RGB值\n",
    "\n",
    "        # 做图片的数据预处理\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # 在这里做transform，转为tensor等等\n",
    "\n",
    "        # 返回图片的张量形式和标签\n",
    "        return img, label\n",
    "\n",
    "    # 一共有多少个样本\n",
    "    # 不然机器没法根据batch_size的个数去确定有几个iteration\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            # 遍历类别\n",
    "            for sub_dir in dirs:\n",
    "                img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "\n",
    "                # 遍历图片\n",
    "                for i in range(len(img_names)):\n",
    "                    img_name = img_names[i]\n",
    "                    path_img = os.path.join(root, sub_dir, img_name)\n",
    "                    label = rmb_label[sub_dir]\n",
    "                    data_info.append((path_img, int(label)))\n",
    "        \n",
    "        # 返回一个list元组，每个元素：[(path_img1, label1),(path_img2, label2)...]\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99be8164",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27676/3970411495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# batch_size：每批样本的个数，本例中每次取16张图片的张量和标签\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# shuffle：打乱顺序\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# map-style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[0;32m    103\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# 数据的路径：从哪里读数据\n",
    "split_dir = os.path.join('data', 'rmb_split')\n",
    "train_dir = os.path.join(split_dir, 'train')\n",
    "valid_dir = os.path.join(split_dir, 'valid')\n",
    "\n",
    "# transforms模块，进行数据预处理\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "# RMBDataset继承抽象类Dataset，并且重写了__getitem__()方法\n",
    "# 这个类的目的就是传入数据的路径和预处理部分，之后返回数据\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "\n",
    "# 构建DataLoader\n",
    "BATCH_SIZE = 16\n",
    "# dataset接收参数RMBDataset，返回一个样本的张量和标签\n",
    "# batch_size：每批样本的个数，本例中每次取16张图片的张量和标签\n",
    "# shuffle：打乱顺序\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 具体训练时，需要便利train_loader，每次取一批数据进行训练\n",
    "# print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 100\n",
    "# 全部样本循环迭代epoch次\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    # 批次循环，每个epoch中都是一批一批训练\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # updata weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0a8df",
   "metadata": {},
   "source": [
    "for i, data in enumerate(train_loader)取数据过程\n",
    "\n",
    "1. 程序第一步跳到DataLoader的\\_\\_iter\\_\\_(self)方法，先判断是多进程还是单进程读取数据。\n",
    "<img style=\"float: center;\" src=\"images/8.png\" width=\"70%\">\n",
    "2. 进入\\_\\_next\\_\\_(self)方法，获取下一个样本与标签\n",
    "  - self.\\_\\_next\\_\\_index()：获取下一个样本的index\n",
    "  - self.dataset_fetcher.fetch(index)：根据index获取下一个样本\n",
    "<img style=\"float: center;\" src=\"images/9.png\" width=\"70%\">\n",
    "3. 返回next(self.sampler_iter)\n",
    "<img style=\"float: center;\" src=\"images/10.png\" width=\"70%\">\n",
    "4. 进入sampler.py，进入\\_\\_iter\\_\\_(self)，一次次采样数据的索引直到够了一个batch_size返回\n",
    "<img style=\"float: center;\" src=\"images/12.png\" width=\"70%\">\n",
    "5. 取到index，之前batch_size设置为16，因此通过sampler.py获取16个样本的索引\n",
    "<img style=\"float: center;\" src=\"images/11.png\" width=\"70%\">\n",
    "6. 这样获取了一个批次的index，之后根据index取数据。因此第二行代码：\n",
    "  - data=self.dataset_fetcher.fetch(index)就是取数据去了\n",
    "7. 进入fetch.py，核心是fetch方法，调用self.dataset[idx]获取数据\n",
    "<img style=\"float: center;\" src=\"images/13.png\" width=\"70%\">\n",
    "8. 之后调用RMBDataset中的\\_\\_getitem\\_\\_(self, index)方法，获取样本，拿到样本的张量和标签， fetch方法是列表推导式，通过该方法能够获取一个batch大小的样本\n",
    "<img style=\"float: center;\" src=\"images/14.png\" width=\"70%\">\n",
    "9. 取完一个批次，进入self.collate_fn(data)进行整合，得到一个批次的data\n",
    "<img style=\"float: center;\" src=\"images/15.png\" width=\"70%\">\n",
    "10. 之后可以看到第一个批次的数据样本，train_loader把样本分成一个个batch，通过enumerate进行迭代就可以一批批地获取，然后训练模型，当所有的批次数据都遍历后，完成一次epoch\n",
    "<img style=\"float: center;\" src=\"images/16.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846c156",
   "metadata": {},
   "source": [
    "三个问题：\n",
    "1. 读哪些数据？根据Sampler输出的index决定的\n",
    "2. 从哪读数据？Dataset的data_dir设置数据的路径然后去读\n",
    "3. 怎么读数据？Dataset的getitem方法，帮助我们获取一个样本\n",
    "\n",
    "流程图：\n",
    "<img style=\"float: center;\" src=\"images/17.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95957f31",
   "metadata": {},
   "source": [
    "# 图像预处理transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69a086",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "transforms是常用的图像预处理方法，在torchvision计算机视觉工具包中。\n",
    "\n",
    "torchvision包主要有三个模块：\n",
    "- torchvision.transforms：常用的图像预处理方法，比如：\n",
    "  - 数据标准化、中心化\n",
    "  - 缩放、裁剪、旋转、翻转、填充\n",
    "  - 噪声添加、灰度变换、线性变换、仿射变换\n",
    "  - 亮度、饱和度、对比度\n",
    "- torchvision.datasets：常用的数据集的dataset实现，MNIST，CIFAR-10，ImageNet等\n",
    "- torchvision.models：常用的模型预训练，AlexNet，VGG，ResNet，GoogLeNet等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d2e7d",
   "metadata": {},
   "source": [
    "## 二分类任务中的transforms方法\n",
    "\n",
    "- transforms.Compose：将一系列的transforms方法进行有序的组合包装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35630b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cef356",
   "metadata": {},
   "source": [
    "- transforms.Compose：将一系列的transforms方法进行有序的组合包装，具体实现时依次用包装的方法对图像进行操作。\n",
    "- transforms.Resize：改变图像大小\n",
    "- transforms.RandomCrop：对图像进行裁剪\n",
    "- transforms.ToTensor：将图像传换成张量，同时会进行归一化，将张量的值从0-255转到0-1\n",
    "- transforms.Normalize：将数据进行标准化\n",
    "\n",
    "流程图：\n",
    "<img style=\"float: center;\" src=\"images/18.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060a882",
   "metadata": {},
   "source": [
    "## 数据标准化\n",
    "\n",
    "transforms.Normalize：逐channel的对图像进行标准化，$output=(input-mean)/std$\n",
    "\n",
    "transforms.Normalize(mean, std, inplace=False)\n",
    "\n",
    "流程图：\n",
    "1. 进入self.transform(img)函数。\n",
    "<img style=\"float: center;\" src=\"images/19.png\" width=\"70%\">\n",
    "2. 进入transform.py，里面的\\_\\_call\\_\\_就是那一系列的数据处理方法\n",
    "<img style=\"float: center;\" src=\"images/20.png\" width=\"70%\">\n",
    "3. 进入Normalize类，里面有一个call函数调用了PyTorch库里的Normalize函数。Normalize处理有利于加快模型的收敛速度。\n",
    "<img style=\"float: center;\" src=\"images/21.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ff990",
   "metadata": {},
   "source": [
    "## transforms的其他图像增强方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697fa35",
   "metadata": {},
   "source": [
    "### 数据增强\n",
    "\n",
    "又称数据增广/数据扩增，对数据集进行变换，使训练集更丰富，从而让模型更具泛化能力。\n",
    "\n",
    "例如，五年高考三年模拟\n",
    "<img style=\"float: center;\" src=\"images/22.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef28709",
   "metadata": {},
   "source": [
    "### 图像裁剪\n",
    "\n",
    "- transforms.CenterCrop(size)：图像中心裁剪图片，size就是所需裁剪的图片尺寸，如果比原始图像大，则会默认填充0\n",
    "- transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')：\n",
    "  - 图像随机位置裁剪size尺寸图片\n",
    "  - padding设置填充大小\n",
    "    - 为a时，上下左右各填充a个像素；\n",
    "    - 为（a，b）时，上下填充b，左右填充a；\n",
    "    - 为（a，b，c，d）时，左上右下分别填充a，b，c，d）\n",
    "  - pad_if_need：若图像小于设定的size，则填充\n",
    "  - padding_mode填充模型：\n",
    "    - constant：像素值由fill设定\n",
    "    - edge：像素值由图像边缘像素设定\n",
    "    - reflect：镜像填充\n",
    "    - symmetric：镜像填充（复制图像的一部分进行填充）\n",
    "- transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(3/4, 4/3), interpolation)：随机大小，长宽比裁剪图像，\n",
    "  - scale：表示随机裁剪面积比例\n",
    "  - ratio：随机长宽比\n",
    "  - interpolation：插值方法\n",
    "- FiveCrop，TenCrop：图像的上下左右及中心裁剪出尺寸为size的5张图片，后者在这5张图片的基础上再水平或者垂直镜像得到10张图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da030aa1",
   "metadata": {},
   "source": [
    "### 图像翻转和旋转\n",
    "- RandomHorizontalFlip(p=0.5)，RandomVerticalFlip(p=0.5)：依概率水平或垂直翻转图片，p为翻转概率\n",
    "- RandomRotation(degrees, resample=False, expand=False, center=None)：随机旋转图片：\n",
    "  - degrees：旋转角度\n",
    "  - resample：重采样方法\n",
    "  - expand：是否扩大图片以保持原图信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532cdd0",
   "metadata": {},
   "source": [
    "### 图像变换\n",
    "\n",
    "- transforms.Pad(padding, fill=0, padding_mode='constant'): 对图片边缘进行填充\n",
    "- transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0):调整亮度、对比度、饱和度和色相：\n",
    "  - brightness：亮度调节因子\n",
    "  - contrast：对比度参数\n",
    "  - saturation：饱和度参数\n",
    "  - hue：色相因子\n",
    "- transfor.RandomGrayscale(num_output_channels, p=0.1)：依概率将图片转换为灰度图，第一个参数是通道数（1或3），p是转换为灰度图像概率值\n",
    "- transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)：对图像进行仿射变换（二维的线性变换），由5种基本原子变换（旋转，平移，缩放，错切和翻转）构成。\n",
    "  - degrees：旋转角度\n",
    "  - translate：平移区间\n",
    "  - scale：缩放比例\n",
    "  - fill_color：填充颜色\n",
    "  - shear：错切\n",
    "- transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)：对图像进行随机遮挡，有利于模型识别被遮挡的图片，这个是对张量进行操作，所以需要先转成张量才能做。\n",
    "  - p：概率值\n",
    "  - scale：遮挡区域面积\n",
    "  - ratio：遮挡区域长宽比\n",
    "  - value：遮挡像素\n",
    "- transforms.Lambda(lambd): 用户自定义的lambda方法，lambd是一个匿名函数。lambda [arg1 [, arg2…argn]]: expression\n",
    "- .Resize, .ToTensor, .Normalize: 这三个方法上面具体说过。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6384c1a",
   "metadata": {},
   "source": [
    "## transforms的选择操作\n",
    "\n",
    "- transforms.RandomChoice([transforms1, transforms2, transforms3]): 从一系列transforms方法中随机选一个\n",
    "- transforms.RandomApply([transforms1, transforms2, transforms3], p=0.5): 依据概率执行一组transforms操作\n",
    "- transforms.RandomOrder([transforms1, transforms2, transforms3]): 对一组transforms操作打乱顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0fa67",
   "metadata": {},
   "source": [
    "## 自定义transforms\n",
    "\n",
    "Compose类里调用一系列transforms方法：\n",
    "<img style=\"float: center;\" src=\"images/23.png\" width=\"70%\">\n",
    "\n",
    "对Compose里的transforms方法进行遍历，每次去除一个方法进行执行，即，**transforms方法仅接收一个参数，返回一个参数**，上一个transforms的输出正好是下一个transforms的输入，因此数据类型要注意匹配。\n",
    "\n",
    "自定义transforms的结构：\n",
    "<img style=\"float: center;\" src=\"images/24.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940328df",
   "metadata": {},
   "source": [
    "## 数据增强策略原则\n",
    "\n",
    "**让训练集与测试机更接近**\n",
    "- 空间位置上：平移\n",
    "- 色彩上：灰度图，色彩抖动\n",
    "- 形状：仿射变换\n",
    "- 上下文场景：遮挡，填充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31d6f3",
   "metadata": {},
   "source": [
    "# 思维导图\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/25.png\" width=\"70%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
