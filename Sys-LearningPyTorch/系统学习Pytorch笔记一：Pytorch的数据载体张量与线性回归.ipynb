{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12859cc",
   "metadata": {},
   "source": [
    "PyTorch是一个Python的科学计算库，它有以下特点：\n",
    "- 类似于Numpy，但是它可以使用GPU\n",
    "- 可以定义深度学习模型，灵活地进行深度学习模型的训练和使用\n",
    "\n",
    "PyTorch官方英文文档：https://pytorch.org/docs/stable/torch.html?\n",
    "\n",
    "PyTorch官方中文文档：https://pytorch-cn.readthedocs.io/zh/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9260930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333172c3",
   "metadata": {},
   "source": [
    "# 张量的简介\n",
    "\n",
    "主要是PyTorch中的数据结构——Tensor\n",
    "\n",
    "Tensor与Numpy中的ndarray类似，唯一的区别是Tensor可以在GPU上加速运算\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/0.png\" width=\"60%\">\n",
    "\n",
    "Tensor一共有8个属性：\n",
    "1. data: 被包装的Tensor\n",
    "2. dtype: 张量的数据类型，用的最多的是float32\n",
    "3. shape: 张量的形状，如(64, 3, 224, 224)\n",
    "4. device: 张量所在的设备，CPU或者GPU，张量只有在GPU上才能加速\n",
    "5. requires_grad: 是否需要梯度\n",
    "6. grad: data的梯度\n",
    "7. grad_fn: fn表示function的意思，记录创建张量时用到的方法，比如加法，乘法，这个操作在求导的过程需要用到，Tensor的Function，是自动求导的关键\n",
    "8. is_leaf: 是否是叶子节点（张量）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865a52e",
   "metadata": {},
   "source": [
    "# 张量的创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9771c0a",
   "metadata": {},
   "source": [
    "## 直接创建\n",
    "由原始数据直接生成张量，张量类型由原始数据类型决定\n",
    "\n",
    "torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)\n",
    "- data：可以是list，也可以是numpy\n",
    "- dtype：数据的类型，默认与data一致\n",
    "- device：所在的设备\n",
    "- requires_grad：是否需要梯度，搭建神经网络的时候需要求导的那些参数要设置为true\n",
    "- pin_memory：是否存于锁页内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b859847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716a30f",
   "metadata": {},
   "source": [
    "## numpy数组创建\n",
    "\n",
    "由已有的numpy数组来生成张量（同时反过来也可以）\n",
    "\n",
    "注意张量和numpy数组在CPU上共用一块内存区域，改变其中一个另一个也会随之改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b92e02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy转tensor\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb4f738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor转numpy\n",
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33d280",
   "metadata": {},
   "source": [
    "## 数值创建\n",
    "\n",
    "新的张量将继承已有的张量的数据属性（结构、类型），也可以重新指定新的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58de66",
   "metadata": {},
   "source": [
    "### 零张量\n",
    "\n",
    "torch.zeros(\\*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "\n",
    "torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f21984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]) \n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "2039376008752 2039376008752 True\n"
     ]
    }
   ],
   "source": [
    "out_t = torch.tensor([1])\n",
    "t = torch.zeros((3, 3), out=out_t)\n",
    "\n",
    "print(out_t, '\\n', t)\n",
    "print(id(t), id(out_t), id(t) == id(out_t))   # 这个看内存地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a3ced0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(out_t)   # 这里的input要是个张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445535c",
   "metadata": {},
   "source": [
    "### 单位张量\n",
    "\n",
    "torch.ones()\n",
    "\n",
    "torch.ones_like()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a21e6a",
   "metadata": {},
   "source": [
    "### 自定义数值张量\n",
    "\n",
    "torch.full()\n",
    "\n",
    "torch.full_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560e20eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10],\n",
       "        [10, 10, 10],\n",
       "        [10, 10, 10]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((3,3), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393890f4",
   "metadata": {},
   "source": [
    "### 范围张量\n",
    "\n",
    "[start, end)，左闭右开\n",
    "\n",
    "torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df14497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6e0de",
   "metadata": {},
   "source": [
    "### 均分张量\n",
    "\n",
    "[start, end]，左闭右闭，$(end-start) / (steps-1)$\n",
    "\n",
    "torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4deaf8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.,  4.,  6.,  8., 10.]),\n",
       " tensor([ 2.0000,  3.6000,  5.2000,  6.8000,  8.4000, 10.0000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(2, 10, 5), torch.linspace(2, 10, 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e383e",
   "metadata": {},
   "source": [
    "### 对数均分张量\n",
    "\n",
    "[start, end]，左闭右闭，$(end-start) / (steps-1)$\n",
    "\n",
    "torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6103726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+02, 1.0000e+04, 1.0000e+06, 1.0000e+08, 1.0000e+10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(2, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24fe82",
   "metadata": {},
   "source": [
    "### 对角矩阵\n",
    "\n",
    "默认是方阵\n",
    "\n",
    "torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4ca60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]),\n",
       " tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3), torch.eye(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9abb0",
   "metadata": {},
   "source": [
    "## 概率分布创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a613378",
   "metadata": {},
   "source": [
    "### 正态分布（高斯分布）\n",
    "\n",
    "torch.normal(mean, std, size, out=None)\n",
    "- mean为标量，std为标量\n",
    "- mean为标量，std为张量\n",
    "- mean为张量，std为标量\n",
    "- mean为张量，std为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a15ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1618,  2.3647, -0.8457,  0.0597])\n",
      "torch.float32\n",
      "tensor([-0.0792, -1.5373, -0.3282,  3.8137])\n",
      "tensor([0.8567, 0.0971, 4.2862, 5.4753])\n",
      "tensor([0.8125, 2.5844, 8.5292, 7.8350])\n"
     ]
    }
   ],
   "source": [
    "# 第一种模式 - 均值是标量，方差是标量 \n",
    "# 此时产生的是一个分布，从这一个分部种抽样相应的个数\n",
    "# 所以这个必须指定size，也就是抽取多少个数\n",
    "t_normal = torch.normal(0, 1, size=(4,))\n",
    "print(t_normal)     # 来自同一个分布\n",
    "\n",
    "# 第二种模式 - 均值是标量，方差是张量\n",
    "# 此时会根据方差的形状大小，产生同样多个分布，每一个分布的均值都是那个标量\n",
    "std = torch.arange(1, 5, dtype=torch.float)\n",
    "print(std.dtype)\n",
    "t_normal2 = torch.normal(1, std)\n",
    "# 也产生来四个数，但是这四个数分别来自四个不同的正态分布，这些分布均值相等\n",
    "print(t_normal2)\n",
    "\n",
    "# 第三种模式 - 均值是张量，方差是标量\n",
    "# 此时也会根据均值的形状大小，产生同样多个方差相同的分布\n",
    "# 从这几个分布中分别取一个值作为结果\n",
    "mean = torch.arange(1, 5, dtype=torch.float)\n",
    "t_normal3 = torch.normal(mean, 1)\n",
    "print(t_normal3)     # 来自不同的分布，但分布里面方差相等\n",
    "\n",
    "# 第四种模式 - 均值是张量，方差是张量\n",
    "# 此时需要均值的个数和方差的个数一样多，分别产生这么多个正态分布，从这里面抽取一个值\n",
    "mean = torch.arange(1, 5, dtype=torch.float)\n",
    "std = torch.arange(1, 5, dtype=torch.float)\n",
    "t_normal4 = torch.normal(mean, std)\n",
    "print(t_normal4)  # 来自不同的分布，各自有自己的均值和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab668a",
   "metadata": {},
   "source": [
    "### 标准正太分布\n",
    "\n",
    "torch.randn(\\*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "\n",
    "torch.randn_like()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6477a",
   "metadata": {},
   "source": [
    "### 均匀分布\n",
    "\n",
    "torch.rand(), torch.rand_like()：在[0, 1)生成均匀分布\n",
    "\n",
    "torch.rand(\\*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "\n",
    "torch.randint(), torch.randint_like()：在[low, hight)生成整数均匀分布\n",
    "\n",
    "torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1d2e2",
   "metadata": {},
   "source": [
    "### 乱序索引\n",
    "\n",
    "生成[0, n-1]的随机排列，n为张量的长度\n",
    "\n",
    "torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982479a4",
   "metadata": {},
   "source": [
    "### 伯努利分布\n",
    "\n",
    "以input为概率，生成伯努利分布（0-1分布，两点分布）\n",
    "\n",
    "torch.bernoulli(input, \\*, generator=None, out=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c842bbd",
   "metadata": {},
   "source": [
    "# 张量的操作\n",
    "\n",
    "张量运算有超过100种。https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "所有这些运算都可以在GPU上运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02413d9",
   "metadata": {},
   "source": [
    "## 基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abac88",
   "metadata": {},
   "source": [
    "### 拼接\n",
    "\n",
    "torch.cat(tensors, dim=0, out=None)：将张量按维度dim进行拼接，tensors为张量序列，dim为拼接维度，只有浮点数可以拼接，long类型无法拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db084dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) torch.Size([2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) torch.Size([4, 3])\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2, 3))\n",
    "print(t, t.shape)\n",
    "\n",
    "t_0 = torch.cat([t, t], dim=0)       # 行拼接\n",
    "t_1 = torch.cat([t, t], dim=1)    # 列拼接\n",
    "print(t_0, t_0.shape)\n",
    "print(t_1, t_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdf0bf",
   "metadata": {},
   "source": [
    "torch.stack(tensors, dim=0, out=None)：新创建的维度dim上进行拼接，tensors表示张量序列，dim为拼接维度，新加了一个Z轴，dim=0时横向看，dim=1时纵向看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7192199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]]) torch.Size([3, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]]) torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t_stack = torch.stack([t,t,t], dim=0)\n",
    "print(t_stack, t_stack.shape)\n",
    "\n",
    "t_stack1 = torch.stack([t, t, t], dim=1)\n",
    "print(t_stack1, t_stack1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bce69",
   "metadata": {},
   "source": [
    "### 切分\n",
    "\n",
    "torch.chunk(input, chunks, dim=0)：将张量按维度dim进行平均切分（向上取整），返回值时张量列表。\n",
    "\n",
    "如果不能整除，最后一份张量小于其他张量，chunks代表一共要切成几份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4f1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), tensor([[1.],\n",
      "        [1.]]))\n",
      "第1个张量：tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第2个张量：tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第3个张量：tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 7))  # 7\n",
    "# 第一个维度切成三块， 那么应该是(2,3), (2,3), (2,1)\n",
    "# 因为7不能整除3，所以每一份应该向上取整，最后不够的有多少算多少\n",
    "list_of_tensors = torch.chunk(a, dim=1, chunks=3)\n",
    "print(list_of_tensors)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(f\"第{idx+1}个张量：{t}, shape is {t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff75fde",
   "metadata": {},
   "source": [
    "torch.split(tensor, split_size_or_sections, dim=0)：将张量按维度dim切分，这个函数比较强大，可以指定切分的长度\n",
    "\n",
    "split_size_or_sections：每一份的长度，为list时，按list元素切分，长度总和必须为维度大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "942cc1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个张量：tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n",
      "第2个张量：tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n",
      "第3个张量：tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2, 5))\n",
    "\n",
    "# [2 , 1, 2]， 这个要保证这个list的大小正好是那个维度的总大小，这样才能切\n",
    "list_of_tensors = torch.split(t, [2, 1, 2], dim=1)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(f\"第{idx+1}个张量：{t}, shape is {t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d89e4",
   "metadata": {},
   "source": [
    "### 索引\n",
    "\n",
    "torch.index_select(input, dim, index, out=None)：维度dim上，按index索引数据，返回值，以index索引数据拼接的张量【按照索引值查找，指定类型long】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ad70a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 0, 4],\n",
      "        [6, 0, 2],\n",
      "        [4, 4, 0]])\n",
      "tensor([[2, 0, 4],\n",
      "        [4, 4, 0]])\n",
      "tensor([[2, 4],\n",
      "        [6, 2],\n",
      "        [4, 0]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0, 9, size=(3, 3))     #  从0-8随机产生数组成3*3的矩阵\n",
    "print(t)\n",
    "\n",
    "idx = torch.tensor([0, 2], dtype=torch.long)   # 这里的类型注意一下，要是long类型\n",
    "t_select = torch.index_select(t, dim=0, index=idx)  #第0行和第2行拼接返回\n",
    "print(t_select)\n",
    "\n",
    "idx = torch.tensor([0, 2], dtype=torch.long)   # 这里的类型注意一下，要是long类型\n",
    "t_select = torch.index_select(t, dim=1, index=idx)  #第0列和第2列拼接返回\n",
    "print(t_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54e0d5",
   "metadata": {},
   "source": [
    "torch.masked_select(input, mask, out=None)：按mask中的True进行索引\n",
    "\n",
    "【按照索引条件查找，指定类型mask】\n",
    "\n",
    "- 返回值：一维张量\n",
    "- input：要索引的张量\n",
    "- mask：与input同形状的布尔类型张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e424e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: \n",
      " tensor([[False, False, False],\n",
      "        [ True, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([6])\n"
     ]
    }
   ],
   "source": [
    "mask = t.ge(5)   # le<=5, ge>=5, gt>5, lt<5\n",
    "print(\"mask: \\n\", mask)\n",
    "\n",
    "t_select1 = torch.masked_select(t, mask)   # 选出t中大于5的元素\n",
    "print(t_select1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393b6ac",
   "metadata": {},
   "source": [
    "### 变换\n",
    "\n",
    "torch.reshape(input, shape)：改变张量的形状\n",
    "- input：源张量\n",
    "- shape：新张量的形状\n",
    "\n",
    "当张量在内存中时连续时，新张量与input共享数据内存，一个改变另一个也改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7da8585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 7, 3, 0, 1, 6, 2, 5])\n",
      "t:tensor([4, 7, 3, 0, 1, 6, 2, 5])\n",
      "t_reshape:\n",
      "tensor([[[4, 7],\n",
      "         [3, 0]],\n",
      "\n",
      "        [[1, 6],\n",
      "         [2, 5]]])\n",
      "t:tensor([1024,    7,    3,    0,    1,    6,    2,    5])\n",
      "t_reshape:\n",
      "tensor([[[1024,    7],\n",
      "         [   3,    0]],\n",
      "\n",
      "        [[   1,    6],\n",
      "         [   2,    5]]])\n",
      "t.data 内存地址:2039392707504\n",
      "t_reshape.data 内存地址:2039392707504\n"
     ]
    }
   ],
   "source": [
    "t = torch.randperm(8)   # randperm是随机排列的一个函数\n",
    "print(t)\n",
    "\n",
    "# -1的话就是根据后面那两个参数，计算出-1这个值，然后再转\n",
    "t_reshape = torch.reshape(t, (-1, 2, 2))\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "\n",
    "t[0] = 1024\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "\n",
    "print(\"t.data 内存地址:{}\".format(id(t.data)))\n",
    "# 这个注意一下，两个是共内存的\n",
    "print(\"t_reshape.data 内存地址:{}\".format(id(t_reshape.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9010d4",
   "metadata": {},
   "source": [
    "torch.transpose(input, dim0, dim1)：交换张量的两个维度，矩阵转置常用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39cb8b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8384, 0.1264, 0.1091, 0.4429],\n",
      "         [0.8656, 0.0335, 0.4584, 0.0982],\n",
      "         [0.0600, 0.6457, 0.3105, 0.0516]],\n",
      "\n",
      "        [[0.8112, 0.8031, 0.5116, 0.6734],\n",
      "         [0.4092, 0.0089, 0.5364, 0.8517],\n",
      "         [0.1873, 0.8088, 0.5605, 0.5071]]])\n",
      "tensor([[[0.8384, 0.8112],\n",
      "         [0.8656, 0.4092],\n",
      "         [0.0600, 0.1873]],\n",
      "\n",
      "        [[0.1264, 0.8031],\n",
      "         [0.0335, 0.0089],\n",
      "         [0.6457, 0.8088]],\n",
      "\n",
      "        [[0.1091, 0.5116],\n",
      "         [0.4584, 0.5364],\n",
      "         [0.3105, 0.5605]],\n",
      "\n",
      "        [[0.4429, 0.6734],\n",
      "         [0.0982, 0.8517],\n",
      "         [0.0516, 0.5071]]])\n",
      "t shape:torch.Size([2, 3, 4])\n",
      "t_transpose shape: torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((2, 3, 4))      # 产生0-1之间的随机数\n",
    "print(t)\n",
    "\n",
    "# c*h*w, w*h*c, 这表示第0维和第2维进行交换\n",
    "t_transpose = torch.transpose(t, dim0=0, dim1=2)\n",
    "print(t_transpose)\n",
    "print(\"t shape:{}\\nt_transpose shape: {}\".format(t.shape, t_transpose.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5112ce",
   "metadata": {},
   "source": [
    "torch.t(input)：2维张量的转置，相当于：torch.transpose(input, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30321ca7",
   "metadata": {},
   "source": [
    "torch.squeeze(input, dim=None, out=None)：压缩长度为1的维度。\n",
    "\n",
    "dim若为None，移除所有长度为1的轴，若指定维度，当且仅当该轴长度为1时可以被移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71861ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((1, 2, 3, 1))\n",
    "t_sq = torch.squeeze(t)\n",
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_1 = torch.squeeze(t, dim=1)\n",
    "\n",
    "print(t.shape)        # torch.Size([1, 2, 3, 1])\n",
    "print(t_sq.shape)     # torch.Size([2, 3])\n",
    "print(t_0.shape)     # torch.Size([2, 3, 1])\n",
    "print(t_1.shape)     # torch.Size([1, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22806d",
   "metadata": {},
   "source": [
    "torch.unsqueeze(input, dim, out=None)：依据dim扩展维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8d94f",
   "metadata": {},
   "source": [
    "## 数学运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1be499",
   "metadata": {},
   "source": [
    "### 标量运算\n",
    "\n",
    "可以分为三大类：\n",
    "\n",
    "- 加减乘除\n",
    " - torch.add()\n",
    " - torch.addcdiv()\n",
    " - torch.addcmul()\n",
    " - torch.sub()\n",
    " - torch.div()\n",
    " - torch.mul()\n",
    "\n",
    "\n",
    "- 对数指数幂\n",
    " - torch.log(input, out=None)\n",
    " - torch.log10(input, out=None)\n",
    " - torch.log2(input, out=None)\n",
    " - torch.exp(input, out=None)\n",
    " - torch.pow()\n",
    "\n",
    "\n",
    "- 三角函数\n",
    " - torch.abs(input, out=None)\n",
    " - torch.acos(input, out=None)\n",
    " - torch.cosh(input, out=None)\n",
    " - torch.cos(input, out=None)\n",
    " - torch.asin(input, out=None)\n",
    " - torch.atan(input, out=None)\n",
    " - torch.atan2(input, out=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbbeb1",
   "metadata": {},
   "source": [
    "torch.add(input, alpha=1, other, out=None)：逐元素计算input+alpha\\*other\n",
    "\n",
    "alpha：乘项因子，权重\n",
    "\n",
    "例如：y=wx+b，可以表示为：torch.add(b, w, x)\n",
    "\n",
    "torch.addcdiv(input, value=1, tensor1, tensor2, out=None)：\n",
    "\n",
    "$out_i=input_i+value\\times \\frac{tensor1_i}{tensor2_i}$\n",
    "\n",
    "torch.addcmul(input, value=1, tensor1, tensor2, out=None)：\n",
    "\n",
    "$out_i=input_i+value\\times tensor1_i\\times tensor2_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "586b522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0:\n",
      "tensor([[-0.4926, -0.9498, -0.4721],\n",
      "        [-0.0935, -0.0196,  0.7111],\n",
      "        [-0.0660, -0.3093,  1.6372]])\n",
      "t_1:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t_add_10:\n",
      "tensor([[ 9.5074,  9.0502,  9.5279],\n",
      "        [ 9.9065,  9.9804, 10.7111],\n",
      "        [ 9.9340,  9.6907, 11.6372]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huris\\AppData\\Local\\Temp/ipykernel_10956/4014563599.py:3: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  t_add = torch.add(t_0, 10, t_1)\n"
     ]
    }
   ],
   "source": [
    "t_0 = torch.randn((3, 3))\n",
    "t_1 = torch.ones_like(t_0)\n",
    "t_add = torch.add(t_0, 10, t_1)\n",
    "\n",
    "print(\"t_0:\\n{}\\nt_1:\\n{}\\nt_add_10:\\n{}\".format(t_0, t_1, t_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4059e",
   "metadata": {},
   "source": [
    "### 向量运算\n",
    "\n",
    "向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或另外一个向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ffda9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.)\n",
      "tensor(5.)\n",
      "tensor(9.)\n",
      "tensor(1.)\n",
      "tensor(362880.)\n",
      "tensor(2.7386)\n",
      "tensor(7.5000)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(1,10).float()\n",
    "\n",
    "print(torch.sum(a))\n",
    "print(torch.mean(a))\n",
    "print(torch.max(a))\n",
    "print(torch.min(a))\n",
    "print(torch.prod(a)) #累乘\n",
    "print(torch.std(a))  #标准差\n",
    "print(torch.var(a))  #方差\n",
    "print(torch.median(a)) #中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da86ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  3,  6, 10, 15, 21, 28, 36, 45])\n",
      "tensor([     1,      2,      6,     24,    120,    720,   5040,  40320, 362880])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.return_types.cummin(\n",
      "values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
      "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "#cum扫描\n",
    "a = torch.arange(1,10)\n",
    "\n",
    "print(torch.cumsum(a,0))    # 累加\n",
    "print(torch.cumprod(a,0))    # 累乘\n",
    "print(torch.cummax(a,0).values)\n",
    "print(torch.cummax(a,0).indices)\n",
    "print(torch.cummin(a,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550af2e9",
   "metadata": {},
   "source": [
    "### 张量排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da7f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[9., 7., 8.],\n",
      "        [5., 6., 4.]]),\n",
      "indices=tensor([[0, 0, 0],\n",
      "        [2, 2, 2]])) \n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([[9., 8.],\n",
      "        [3., 2.],\n",
      "        [6., 5.]]),\n",
      "indices=tensor([[0, 2],\n",
      "        [1, 2],\n",
      "        [1, 0]])) \n",
      "\n",
      "torch.return_types.sort(\n",
      "values=tensor([[7., 8., 9.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 5., 6.]]),\n",
      "indices=tensor([[1, 2, 0],\n",
      "        [0, 2, 1],\n",
      "        [2, 0, 1]])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#torch.sort和torch.topk可以对张量排序\n",
    "a = torch.tensor([[9,7,8],[1,3,2],[5,6,4]]).float()\n",
    "print(torch.topk(a,2,dim = 0),\"\\n\")\n",
    "print(torch.topk(a,2,dim = 1),\"\\n\")\n",
    "print(torch.sort(a,dim = 1),\"\\n\")\n",
    "\n",
    "#利用torch.topk可以在Pytorch中实现KNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f799ec",
   "metadata": {},
   "source": [
    "### 矩阵运算\n",
    "\n",
    "矩阵必须是二维的，类似torch.tensor([1, 2, 3])不是矩阵\n",
    "\n",
    "矩阵的运算包括：\n",
    "- 乘法\n",
    "- 转置\n",
    "- 求逆\n",
    "- 求迹（nxn矩阵主对角线上元素之和）\n",
    "- 范数\n",
    "- 行列式\n",
    "- 特征值/向量\n",
    "- 分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "279c6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4],\n",
      "        [6, 8]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵乘法\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[2,0],[0,2]])\n",
    "print(a@b)  #等价于torch.matmul(a,b) 或 torch.mm(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32bacf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵转置\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(a.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb42b80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵逆，必须为浮点类型\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.inverse(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44438b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "#矩阵求trace\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.trace(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cedd1c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4772)\n"
     ]
    }
   ],
   "source": [
    "#矩阵求范数\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be2643f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.0000)\n"
     ]
    }
   ],
   "source": [
    "#矩阵行列式\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaf69d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[ 2.5000,  2.7839],\n",
      "        [ 2.5000, -2.7839]]),\n",
      "eigenvectors=tensor([[ 0.2535, -0.4706],\n",
      "        [ 0.8452,  0.0000]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huris\\AppData\\Local\\Temp/ipykernel_10956/737208349.py:3: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2894.)\n",
      "  print(torch.eig(a,eigenvectors=True))\n"
     ]
    }
   ],
   "source": [
    "#矩阵特征值和特征向量\n",
    "a = torch.tensor([[1.0,2],[-5,4]],dtype = torch.float)\n",
    "print(torch.eig(a,eigenvectors=True))\n",
    "\n",
    "#两个特征值分别是 2.5+2.7839j, 2.5-2.7839j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cd4ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3162, -0.9487],\n",
      "        [-0.9487,  0.3162]]) \n",
      "\n",
      "tensor([[-3.1623, -4.4272],\n",
      "        [ 0.0000, -0.6325]]) \n",
      "\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 4.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huris\\AppData\\Local\\Temp/ipykernel_10956/3533039936.py:5: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1937.)\n",
      "  q, r = torch.qr(a)\n"
     ]
    }
   ],
   "source": [
    "#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r\n",
    "#QR分解实际上是对矩阵a实施Schmidt正交化得到q\n",
    "\n",
    "a = torch.tensor([[1.0,2.0],[3.0,4.0]])\n",
    "q, r = torch.qr(a)\n",
    "print(q,\"\\n\")\n",
    "print(r,\"\\n\")\n",
    "print(q@r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3976eb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2298,  0.8835],\n",
      "        [-0.5247,  0.2408],\n",
      "        [-0.8196, -0.4019]]) \n",
      "\n",
      "tensor([9.5255, 0.5143]) \n",
      "\n",
      "tensor([[-0.6196, -0.7849],\n",
      "        [-0.7849,  0.6196]]) \n",
      "\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 4.0000],\n",
      "        [5.0000, 6.0000]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵svd分解\n",
    "#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积\n",
    "#svd常用于矩阵压缩和降维\n",
    "a=torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "\n",
    "u,s,v = torch.svd(a)\n",
    "\n",
    "print(u,\"\\n\")\n",
    "print(s,\"\\n\")\n",
    "print(v,\"\\n\")\n",
    "\n",
    "print(u@torch.diag(s)@v.t())\n",
    "\n",
    "#利用svd分解可以在Pytorch中实现主成分分析降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87ac79",
   "metadata": {},
   "source": [
    "### 自动赋值运算\n",
    "\n",
    "自动赋值运算通常在方法后有`_`作为后缀，例如：`x.copy_(y)`,`x.t_()`的操作会改变`x`的取值。\n",
    "\n",
    "注意，自动赋值运算虽然可以节省内存，但是在求导时会因为丢失中间过程而导致一些问题，一般不鼓励使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b437a289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
      "\n",
      "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(10)\n",
    "print(tensor, \"\\n\")\n",
    "\n",
    "tensor.add(5)\n",
    "print(tensor, \"\\n\")\n",
    "\n",
    "tensor.add_(5)\n",
    "print(tensor, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454ec2e",
   "metadata": {},
   "source": [
    "# 线性回归例子\n",
    "\n",
    "分析一个变量与另外一（多）个变量之间关系的方法。\n",
    "\n",
    "- 因变量：y\n",
    "- 自变量：x\n",
    "- 关系线性：$y=w\\times x+b$\n",
    "\n",
    "任务就是求解w，b\n",
    "\n",
    "求解步骤：\n",
    "1. 确定模型：Model->y=wx+b\n",
    "2. 确定损失函数：$Loss=\\frac{1}{m}\\sum^m_{i=1}(y_i-\\hat{y_i})^2$\n",
    "3. 求解梯度更新w，b：$w=w-lr\\times w.grad$，$b=b-lr\\times b.grad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b3b1d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.1419]), tensor([4.1725]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "true_w = 2\n",
    "true_b = 5\n",
    "lr = 0.001\n",
    "\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = true_w * x + (true_b + torch.randn(20, 1))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "for iteration in range(10000):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x)\n",
    "    y_pred = torch.add(b, wx)\n",
    "\n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y - y_pred) ** 2).mean()\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "w.data, b.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad00ac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dadc7b2dc0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "d2l.plt.scatter(x.detach().numpy(), y.detach().numpy(), s=10, c='r')\n",
    "d2l.plt.plot(x.detach().numpy(), w.data * x + b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42e1e7",
   "metadata": {},
   "source": [
    "# 思维导图\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/1.png\" width=\"90%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
