{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716f0ed",
   "metadata": {},
   "source": [
    "PyTorch神经网络建模：\n",
    "- 数据准备\n",
    "- 模型建立\n",
    "- 模型训练\n",
    "- 模型评估使用和保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd2107",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "\n",
    "数据准备是PyTorch网络中非常重要且比较困难的一个部分，实际任务中一般会面临结构化数据、图片数据、文本数据和时间序列数据，不同的数据会有不同的数据准备方法。\n",
    "\n",
    "本文总结针对这四种数据进行准备，每种数据又有哪些不同的建模方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d7db2",
   "metadata": {},
   "source": [
    "# Dataset与DataLoader\n",
    "\n",
    "PyTorch通常使用Dataset和DataLoader这两个工具类构建数据管道，建模各种数据避不开这两个工具类。\n",
    "\n",
    "- Dataset定义数据集的内容，类似于列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。\n",
    "  - 绝大多数情况下，只需实现Dataset的\\_\\_len\\_\\_方法和\\_\\_getitem\\_\\_方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载\n",
    "- DataLoader定义按batch加载数据集的方法，实现一个\\_\\_iter\\_\\_方法的可迭代对象，每次迭代输出一个batch数据。\n",
    "  - DataLoader可以控制batch大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，同时可以多进程读取数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6574c",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "通过这两个工具类完成数据管道的构建，有两个问题：\n",
    "- 如何获取一个batch？\n",
    "- Dataset与DataLoader是如何分工合作的？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59da309",
   "metadata": {},
   "source": [
    "### 如何获取一个batch\n",
    "\n",
    "数据集特征X，标签Y，则数据集可以表示成（X，Y），batch大小为m，则获取一个batch步骤：\n",
    "- 确定数据集的长度n\n",
    "  - 一共有多少个样本，这样指定每个batch时，计算机可以规划分为几个batch\n",
    "- 从[0, n-1]范围内抽样m个数（batch大小）\n",
    "  - 假设m=4，则拿到的结果就是一个列表，类似：indics=[1, 4, 8, 9]\n",
    "- 根据下表从数据集中取m个数对应的下标元素\n",
    "  - 拿到一个元组列表，类似：samples=[(X[1], Y[1]), (X[4], Y[4]), (X[8], Y[8]), (X[9], Y[9])]\n",
    "- 将结果整理成两个张量作为输出：\n",
    "  - 结果时两个张量，类似：batch=(features, labels)，其中：\n",
    "  - features=torch.stack(X[1], X[4], X[8], X[9])\n",
    "  - labels=torch.stack(Y[1], Y[4], Y[8], Y[9])\n",
    "\n",
    "这样，就完成了一个batch数据获取操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038f4d4",
   "metadata": {},
   "source": [
    "### Dataset与DataLoader功能分工\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/1.png\" width=\"70%\">\n",
    "\n",
    "上述流程图，把DataLoader读取数据的流程梳理一遍。\n",
    "\n",
    "DataLoader的作用就是构建一个数据装载器，根据提供的batch_size大小，将数据样本分成一个个batch去训练模型，这个分的过程中需要把数据取到（需要借助Dataset的getitem）：\n",
    "- 第一个步骤：数据的总长度，这个需要在Dataset的\\_\\_len\\_\\_方法中告诉计算机\n",
    "- 第二个步骤：0到n-1范围中抽样出m个数的方法，由DataLoader的sampler和batch_sampler参数指定：\n",
    "  - sampler参数指定单个元素抽样方法（一般无需设置），程序默认在DataLoader的参数shuffle=True时采用随机抽样，shuffle=False时采用顺序抽样。\n",
    "  - batch_sampler参数将多个抽样的元素整理成一个列表（一般无需设置），默认方法在DataLoader的参数drop_last=True时会丢弃数据集最后一个长度不能被batch大小整除的批次，在drop_last=False时保留最后一个批次。\n",
    "- 第三个步骤：根据下标取数据集中的元素（需要自己写读取函数），由Dataset的\\_\\_getitem\\_\\_方法实现，这个函数接收的参数是一个索引。\n",
    "- 第四个步骤：逻辑由DataLoader的参数collate_fn指定，一般无需设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567969e",
   "metadata": {},
   "source": [
    "## 使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aaa370",
   "metadata": {},
   "source": [
    "### Dataset创建数据集\n",
    "\n",
    "Dataset核心接口逻辑伪代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe58ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45243467",
   "metadata": {},
   "source": [
    "Dataset创建数据集常用方法：\n",
    "- torch.utils.data.TensorDataset：根据Tensor创建数据集（Numpy的array和Pandas的DataFrame需要先转换成Tensor）\n",
    "- torchvision.datasets.ImageFolder：根据图片目录创建图片数据集\n",
    "- torch.utils.data.Dataset：创建自定义数据集（需要实现len和getitem方法）\n",
    "- torch.utils.data.random_split：将一个数据集分割成多份（训练集，验证集，测试机）\n",
    "- 调用Dataset加法运算符（+）将多个数据集合并成一个数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fd761",
   "metadata": {},
   "source": [
    "### DataLoader加载数据集\n",
    "\n",
    "DataLoader逻辑接口代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fc96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False):\n",
    "        self.dataset = dataset\n",
    "        self.sampler =torch.utils.data.RandomSampler if shuffle else \\\n",
    "           torch.utils.data.SequentialSampler\n",
    "        self.batch_sampler = torch.utils.data.BatchSampler\n",
    "        self.sample_iter = self.batch_sampler(\n",
    "            self.sampler(range(len(dataset))),\n",
    "            batch_size = batch_size,drop_last = drop_last)\n",
    "        \n",
    "    def __next__(self):\n",
    "        indices = next(self.sample_iter)\n",
    "        batch = self.collate_fn([self.dataset[i] for i in indices])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b897e",
   "metadata": {},
   "source": [
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所输入形式的方法，并且能够使用多进程读取数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26eaa6",
   "metadata": {},
   "source": [
    "DataLoader的函数签名如下：\n",
    "```python\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n",
    "\n",
    "通常仅配置dataset, batch_size, shuffle, num_workers, drop_last五个参数，其他参数使用默认值即可。\n",
    "\n",
    "DataLoader除了可以加载torch.utils.data.Dataset外，还可以加载另外一种数据集：\n",
    "- torch.utils.data.IterableDataset\n",
    "- Dataset数据集相当于一种列表结构，IterableDataset相当于一种迭代器结构，其更加复杂，一般较少使用。\n",
    "- 参数说明：\n",
    "  - dataset：数据集\n",
    "  - batch_size：批次大小\n",
    "  - shuffle：是否乱序\n",
    "  - sampler：样本采样函数，一般无需设置\n",
    "  - batch_sampler：批次采样函数，一般无需设置\n",
    "  - num_workers：使用多进程读取数据，设置的进程数\n",
    "  - collate_fn：整理一个批次数据的函数。\n",
    "  - pin_memory：是否设置为锁业内存。默认False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。\n",
    "  - drop_last：是否丢弃最后一个样本数量不足batch_size批次数据。\n",
    "  - timeout：加载一个数据批次的最长等待时间，一般无需设置。\n",
    "  - worker_init_fn：每个worker中dataset的初始化函数，常用于IterableDataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928e6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49, 14, 33,  8, 22, 29, 28, 20, 27, 19])\n",
      "tensor([ 7, 26, 25, 48,  6, 36,  5, 12, 45, 46])\n",
      "tensor([ 9, 13, 17,  1, 24, 39, 11, 15, 16, 32])\n",
      "tensor([47, 44, 21, 31, 23, 34, 35, 43,  4, 38])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as tud\n",
    "import torch\n",
    "\n",
    "#构建输入数据管道\n",
    "ds = tud.TensorDataset(torch.arange(1,50))\n",
    "dl = tud.DataLoader(ds,\n",
    "                batch_size = 10,\n",
    "                shuffle= True,\n",
    "                num_workers=2,\n",
    "                drop_last = True)\n",
    "#迭代数据\n",
    "for batch, in dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb0710",
   "metadata": {},
   "source": [
    "# 结构化数据建模\n",
    "\n",
    "将结构化数据封装成DataLoader形式，形成可迭代的数据管道，模型训练时，可以遍历DataLoader对象去取数据。\n",
    "\n",
    "结构化数据有三种建模方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730091cb",
   "metadata": {},
   "source": [
    "## 直接用TensorDataset和DataLoader封装成可迭代的数据管道\n",
    "\n",
    "只要把结构化的数据预处理完毕，做成训练集和测试集之后，直接拿这两个函数进行封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667a2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 首先， 读入数据\n",
    "dftrain_raw = pd.read_csv('data/titanic/train.csv')\n",
    "dftest_raw = pd.read_csv('data/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42631b05",
   "metadata": {},
   "source": [
    "可以看到数据的样子，典型的结构化数据：\n",
    "<img style=\"float: center;\" src=\"images/2.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3462196",
   "metadata": {},
   "source": [
    "进行简单的数据预处理，划分出训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0ef1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dfdata):\n",
    "    dfresult= pd.DataFrame()\n",
    "    #Pclass\n",
    "    dfPclass = pd.get_dummies(dfdata['Pclass'])\n",
    "    dfPclass.columns = ['Pclass_' +str(x) for x in dfPclass.columns ]\n",
    "    dfresult = pd.concat([dfresult,dfPclass],axis = 1)\n",
    "\n",
    "    #Embarked\n",
    "    dfEmbarked = pd.get_dummies(dfdata['Embarked'],dummy_na=True)\n",
    "    dfEmbarked.columns = ['Embarked_' + str(x) for x in dfEmbarked.columns]\n",
    "    dfresult = pd.concat([dfresult,dfEmbarked],axis = 1)\n",
    "\n",
    "    return(dfresult)\n",
    "\n",
    "x_train = preprocessing(dftrain_raw).values   # x_train.shape = (712, 15)\n",
    "y_train = dftrain_raw.Survived.values    # y_train.shape = (712, 1)\n",
    "\n",
    "x_test = preprocessing(dftest_raw).values   # x_test.shape = (179, 15)\n",
    "y_test = dftest_raw.Survived.values     # y_test.shape = (179, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcee71d",
   "metadata": {},
   "source": [
    "数据处理完毕后，可以发现训练集样本是712,特征数是15，之后就可以构建数据管道了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e43159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0.]]) tensor([1., 0., 1., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "dl_train = tud.DataLoader(tud.TensorDataset(torch.tensor(x_train).float(),torch.tensor(y_train).float()),\n",
    "                     shuffle = True, batch_size = 8)\n",
    "dl_valid = tud.DataLoader(tud.TensorDataset(torch.tensor(x_test).float(),torch.tensor(y_test).float()),\n",
    "                     shuffle = False, batch_size = 8)\n",
    "\n",
    "# 我们可以测试一下数据管道\n",
    "for features,labels in dl_train:\n",
    "    print(features,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad71ecf",
   "metadata": {},
   "source": [
    "## 自己构造数据管道迭代器，不通过Pytorch提供的接口\n",
    "\n",
    "自己写如何划分数据，首先获取全部数据的个数，然后生成索引，根据提供的batch大小把索引划分开，返回对应的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1c3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 样本数量\n",
    "n = 400\n",
    "\n",
    "# 生成测试用数据集\n",
    "X = 10 * torch.rand([n, 2]) - 5.0  #torch.rand是均匀分布 \n",
    "w0 = torch.tensor([[2.0], [-3.0]])\n",
    "b0 = torch.tensor([[10.0]])\n",
    "Y = X @ w0 + b0 + torch.normal(0.0, 2.0, size = [n, 1])  # @表示矩阵乘法,增加正态扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cf45c",
   "metadata": {},
   "source": [
    "数据集有400个样本，2个特征，构建数据管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3426b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(features, labels, batch_size=8):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        indexs = torch.LongTensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield  features.index_select(0, indexs), labels.index_select(0, indexs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf637c18",
   "metadata": {},
   "source": [
    "模型训练的时候，直接遍历即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f8b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4392, -4.0170],\n",
      "        [ 2.3910,  2.2930],\n",
      "        [ 3.6774,  2.5477],\n",
      "        [ 1.5018,  0.4600],\n",
      "        [ 2.1547,  0.6044],\n",
      "        [ 4.5885, -0.9907],\n",
      "        [ 4.5781,  4.5254],\n",
      "        [-2.4320,  2.4013]])\n",
      "tensor([[19.4501],\n",
      "        [ 9.5968],\n",
      "        [12.1262],\n",
      "        [13.4225],\n",
      "        [11.8993],\n",
      "        [22.1154],\n",
      "        [ 7.3442],\n",
      "        [-0.8488]])\n"
     ]
    }
   ],
   "source": [
    "for feature, label in data_iter(X, Y, batch_size=8):\n",
    "    print(feature)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacafe5",
   "metadata": {},
   "source": [
    "# 时序数据建模\n",
    "\n",
    "时序数据建模需要自定义数据集，涉及自回归问题，也就是用到了历史的数据作为特征。\n",
    "\n",
    "这种数据一般会采用滑动窗口把原数据集进行一个切割获得X和Y。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186ff52",
   "metadata": {},
   "source": [
    "下面这个数据集是中国2020年3月之前的疫情数据，典型的时间序列数据：\n",
    "<img style=\"float: center;\" src=\"images/3.png\" width=\"70%\">\n",
    "\n",
    "特征是确诊人数，治愈人数和死亡人数，预测是接下来的确诊人数，治愈人数和死亡人数。（自回归）\n",
    "\n",
    "通过继承torch.utils.data.Dataset实现自定义事件序列数据集：\n",
    "- \\_\\_len\\_\\_：实现len(dataset)返回整个数据集的大小\n",
    "- \\_\\_getitem\\_\\_：获取一些索引数据，使用dataset[i]返回数据集中的第i个样本\n",
    "\n",
    "必须要覆盖这两个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6985337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用某日前8天窗口数据作为输入预测该日数据\n",
    "WINDOW_SIZE = 8\n",
    "\n",
    "class Covid19Dataset(tud.Dataset):\n",
    "    def __len__(self):\n",
    "        return len(dfdiff) - WINDOW_SIZE\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        x = dfdiff.loc[i:i+WINDOW_SIZE-1,:]\n",
    "        feature = torch.tensor(x.values)\n",
    "        y = dfdiff.loc[i+WINDOW_SIZE,:]\n",
    "        label = torch.tensor(y.values)\n",
    "        return (feature,label)\n",
    "    \n",
    "ds_train = Covid19Dataset()\n",
    "\n",
    "#数据较小，可以将全部训练数据放入到一个batch中，提升性能\n",
    "dl_train = tud.DataLoader(ds_train, batch_size = 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ebe45",
   "metadata": {},
   "source": [
    "这是比较常规的时序建模方式，下面是一个非线性自回归的一个自定义数据集的方式，更加复杂一些。\n",
    "\n",
    "非线性自回归建模：既涉及到普通的结构化数据，也涉及到时序数据。\n",
    "<img style=\"float: center;\" src=\"images/4.png\" width=\"70%\">\n",
    "\n",
    "要预测未来7天的温度，既涉及到温度本身的特征，也涉及到其他特征来影响温度，问题的建模思路与上面一致，基于过去一段时间的所有特征，但是写法与上面不同，不是直接一个滑动窗口去切，而是先把这些滞后特征做成一行，然后再进行shape转换。\n",
    "\n",
    "这个比较复杂的就是滞后特征的提取，这个灵活性更高，可以在series_to_supervised函数里做各种处理工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f55f5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dsw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20084/1239793094.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mdata_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTemperatureDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20084/1239793094.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_in, n_out)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \"\"\"\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# 读取数据，去除缺失严重属性，插值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dsw.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sEMG\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dsw.csv'"
     ]
    }
   ],
   "source": [
    "class TemperatureDataSet(Dataset):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        函数用途： 加载数据并且初始化\n",
    "        参数说明：\n",
    "            n_in: 用多长时间进行预测\n",
    "            n_out: 预测多长时间\n",
    "        \"\"\"\n",
    "        # 读取数据，去除缺失严重属性，插值\n",
    "        df = pd.read_csv(\"dsw.csv\", index_col=0)\n",
    "        df.drop(['pH'], axis=1, inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.interpolate(method='time', inplace=True)  # 时间插值\n",
    "\n",
    "        # 标准化\n",
    "        self.ss = StandardScaler()\n",
    "        self.std_data = self.ss.fit_transform(df['Temperature'].values.reshape(-1, 1))\n",
    "\n",
    "        # 转化为监督测试集\n",
    "        data = self.series_to_supervised(self.std_data, n_in, n_out)\n",
    "        re_data = self.series_to_supervised(df['Temperature'].values.reshape(-1, 1), n_in, n_out)\n",
    "        self.x = torch.from_numpy(data.iloc[:, :n_in].values).view(-1, n_in, 1)\n",
    "        self.y = torch.from_numpy(re_data.iloc[:, n_in:].values).view(-1, n_out, 1)\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item, :].type(torch.FloatTensor), self.y[item, :].type(torch.FloatTensor)\n",
    "\n",
    "    def series_to_supervised(self, data, n_in=1, n_out=1, dropnan=True):\n",
    "        \"\"\"\n",
    "        函数用途：将时间序列转化为监督学习数据集。\n",
    "        参数说明：\n",
    "            data: 观察值序列，数据类型可以是 list 或者 NumPy array。\n",
    "            n_in: 作为输入值(X)的滞后组的数量。\n",
    "            n_out: 作为输出值(y)的观察组的数量。\n",
    "            dropnan: Boolean 值，确定是否将包含 NaN 的行移除。\n",
    "        返回值:\n",
    "            经过转换的用于监督学习的 Pandas DataFrame 序列。\n",
    "        \"\"\"\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        df = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "        # 输入序列 (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "        # 预测序列 (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):dsw.csv\n",
    "            cols.append(df.shift(-i))\n",
    "            if i == 0:\n",
    "                names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "        # 将所有列拼合\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop 掉包含 NaN 的行\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "\n",
    "data_set = TemperatureDataSet(n_in=10, n_out=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a191d3b",
   "metadata": {},
   "source": [
    "# 图片数据建模\n",
    "\n",
    "PyTorch中构建图片数据管道通常有两种方法：\n",
    "1. torchvision中的datasets.ImageFolder读取图片然后用DataLoader来并行加载。\n",
    "2. 继承torch.utils.data.Dataset实现用户自定义读取逻辑然后用DataLoader并行加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c96e5f",
   "metadata": {},
   "source": [
    "## torchvision中的datasets.ImageFloder\n",
    "\n",
    "torchvision是一个计算机视觉工具包，需要在安装PyTorch后单独安装这个包，其有三个模块：\n",
    "- torchvision.transforms：常用的图像预处理方法，比如标准化、中心化、旋转、翻转等操作\n",
    "- torchvision.datasets：常用的数据集dataset实现，MNIST，CIFAR-10，ImageNet等\n",
    "- torchvision.models：常用模型预训练，AlexNet，VGG，ResNet，GoogLeNet\n",
    "\n",
    "这里以CIFAR-2数据集作为示例，训练集有airplane和automobile图片各5000张，测试集照片各1000张，任务的目标是训练一个模型来对airplane和automobile进行分类。\n",
    "<img style=\"float: center;\" src=\"images/5.png\" width=\"70%\">\n",
    "\n",
    "使用ImageFloder读取数据集，先定义数据的转换格式，图片数据涉及各种数据预处理，比如转成张量，裁剪，旋转，变换等操作，先定义这些操作，之后在读取数据集的时候，通过transform参数把这些处理传入，就可以直接处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6654654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预处理方式\n",
    "# Compose里可以放更多的处理方式\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "transform_valid = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "# 读取数据\n",
    "ds_train = datasets.ImageFolder(\"./data/cifar2/train/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "ds_valid = datasets.ImageFolder(\"./data/cifar2/test/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    " \n",
    "# 封装成数据管道\n",
    "dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True,num_workers=3)\n",
    "dl_valid = DataLoader(ds_valid,batch_size = 50,shuffle = True,num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2ac1c",
   "metadata": {},
   "source": [
    "## 用户自定义读取逻辑用DataLoader加载\n",
    "\n",
    "这里数据形式与上面一样：\n",
    "<img style=\"float: center;\" src=\"images/6.png\" width=\"70%\">\n",
    "\n",
    "类别分开，每一类里都是图片数据，首先定义一个自己写的Dataset类读取数据，生成数据集，实现逻辑依然是\\_\\_getitem\\_\\_里可以拿到某个索引对应的数据，注意这个函数接收的参数是一个索引，返回这个索引对应的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMBDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        rmb面额分类任务的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        self.label_name = {\"1\": 0, \"100\": 1}\n",
    "        self.data_info = self.get_img_info(data_dir)  # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_img, label = self.data_info[index]\n",
    "        img = Image.open(path_img).convert('RGB')     # 0~255\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # 在这里做transform，转为tensor等等\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            # 遍历类别\n",
    "            for sub_dir in dirs:\n",
    "                img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "\n",
    "                # 遍历图片\n",
    "                for i in range(len(img_names)):\n",
    "                    img_name = img_names[i]\n",
    "                    path_img = os.path.join(root, sub_dir, img_name)\n",
    "                    label = rmb_label[sub_dir]\n",
    "                    data_info.append((path_img, int(label)))\n",
    "\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3f660",
   "metadata": {},
   "source": [
    "之后用自定义的Datasets，构建数据管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07088f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms模块，进行数据预处理\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "## 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "\n",
    "# 构建DataLoader\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6d6b7",
   "metadata": {},
   "source": [
    "# 文本数据建模\n",
    "\n",
    "文本数据预处理较为繁琐，包括中文切词，构建词典，编码转换，序列填充，构建数据管道等等。\n",
    "\n",
    "PyTorch中对文本数据建模一般采用两种方式：\n",
    "- torchtext包：可以构建文本分类，序列标注，问答模型，机器翻译等NLP任务数据集\n",
    "- 自定义Dataset，处理比较繁琐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356f630",
   "metadata": {},
   "source": [
    "IMDB数据集的目标是根据电影评论的文本内容预测评论的情感标签：\n",
    "- 训练集有2w条电影评论文本\n",
    "- 测试集有5k条电影评论文本\n",
    "- 其中正负评论各占一半\n",
    "<img style=\"float: center;\" src=\"images/7.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108981f",
   "metadata": {},
   "source": [
    "## torchtext包进行文本分类数据建模\n",
    "\n",
    "- torchtext.data.Example：表示一个样本，数据和标签\n",
    "- torchtext.vocab.Vocab：词汇表，可以导入一些预训练词向量\n",
    "- torchtext.data.Datasets：数据集类，\\_\\_getitem\\_\\_返回Example实例。\n",
    "- torchtext.data.Field：定义字段的处理方法（文本字段，标签字段）创建 Example时的预处理，batch时的一些处理操作。\n",
    "- torchtext.data.Iterator：迭代器，生成batch\n",
    "- torchtext.datasets：包含了常见的数据集\n",
    "\n",
    "操作逻辑，先从Field里定义一些字段预处理方式，然后用Datasets里的子类构建数据集，构建词典，把数据用Iterator封装成数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dab191",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "# string.punctuation表示所有的标点字符， 下面这句话就是把每个句子里面的所有标点符号都替换成空字符， 然后按照空格进行分词\n",
    "tokenize = lambda x: re.sub('[%s]' % string.punctuation, \"\", x).split(\" \")  # 字符串的替换\n",
    "\n",
    "def filterLowFreqWords(arr, vocab):\n",
    "    arr = [[x if x<MAX_WORDS else 0 for x in example] for example in arr]\n",
    "    return arr\n",
    "\n",
    "# 定义各个字段的预处理方法\n",
    "# sequential告诉它输入是序列的形式， lower等于True， 转成小写  postprocessing=False这块不知道啥意思\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenize, lower=True, fix_length=MAX_LEN, postprocessing=filterLowFreqWords)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)   \n",
    "#field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。这个映射被称为vocab。\n",
    "#如果一个field已经被数字化了并且不需要被序列化，可以将参数设置为use_vocab=False以及sequential=False。\n",
    "\n",
    "\n",
    "# 构建表格型dataset  \n",
    "# torchtext.data.TabularDataset可读取csv, tsv, json等格式\n",
    "ds_train, ds_test = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/imdb', train='train.tsv', test='test.tsv', format='tsv',\n",
    "    fields=[('label', LABEL), ('text', TEXT)], skip_header=False\n",
    ")\n",
    "\n",
    "# 构建词典\n",
    "TEXT.build_vocab(ds_train)\n",
    "\n",
    "# 构建数据管道迭代器\n",
    "train_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "    (ds_train, ds_test), sort_within_batch=True, sort_key=lambda x: len(x.text),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE)\n",
    ")\n",
    "# 查看数据\n",
    "for batch in train_iter:\n",
    "    features = batch.text\n",
    "    labels = batch.label\n",
    "    print(features)\n",
    "    print(features.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5f578",
   "metadata": {},
   "source": [
    "## 自定义文本数据集\n",
    "\n",
    "思路：先对训练文本分词构建词典，然后将训练集文本和测试集文本数据转换成token单词编码，之后转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。最后根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dadef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些变量\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "train_data_path = 'data/imdb/train.tsv'\n",
    "test_data_path = 'data/imdb/test.tsv'\n",
    "train_token_path = 'data/imdb/train_token.tsv'\n",
    "test_token_path =  'data/imdb/test_token.tsv'\n",
    "train_samples_path = 'data/imdb/train_samples/'\n",
    "test_samples_path =  'data/imdb/test_samples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da1b23",
   "metadata": {},
   "source": [
    "我们构建词典， 并保留最高频个词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##构建词典\n",
    "word_count_dict = {}    \n",
    "\n",
    "#清洗文本\n",
    "def clean_text(text):\n",
    "    lowercase = text.lower().replace(\"\\n\",\" \")   # 转成小写\n",
    "    stripped_html = re.sub('<br />', ' ',lowercase)    # 正则修改\n",
    "    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation),'',stripped_html)  # 去掉其他符号\n",
    "    return cleaned_punctuation\n",
    "\n",
    "with open(train_data_path,\"r\",encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        label,text = line.split(\"\\t\")     # label和句子分开\n",
    "        cleaned_text = clean_text(text)      # 清洗文本\n",
    "        for word in cleaned_text.split(\" \"):\n",
    "            word_count_dict[word] = word_count_dict.get(word,0)+1  # 统计单词频数\n",
    "\n",
    "df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = \"count\"))\n",
    "df_word_dict = df_word_dict.sort_values(by = \"count\",ascending =False)\n",
    "\n",
    "df_word_dict = df_word_dict[0:MAX_WORDS-2] #  \n",
    "df_word_dict[\"word_id\"] = range(2,MAX_WORDS) #编号0和1分别留给未知词<unkown>和填充<padding>\n",
    "\n",
    "word_id_dict = df_word_dict[\"word_id\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246cbbf",
   "metadata": {},
   "source": [
    "利用构建好的词典，将文本转成token序号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换token\n",
    "# 填充文本\n",
    "def pad(data_list,pad_length):\n",
    "    padded_list = data_list.copy()\n",
    "    if len(data_list)> pad_length:    # 如果句子里面的单词个数比字典长度大， 那就取后面字典长度个大小\n",
    "         padded_list = data_list[-pad_length:]\n",
    "    if len(data_list)< pad_length:   # 如果句子小， 前面填充1， 弄到字典长度大小\n",
    "         padded_list = [1]*(pad_length-len(data_list))+data_list\n",
    "    return padded_list\n",
    "\n",
    "def text_to_token(text_file,token_file):\n",
    "    with open(text_file,\"r\",encoding = 'utf-8') as fin,\\\n",
    "      open(token_file,\"w\",encoding = 'utf-8') as fout:\n",
    "        for line in fin:\n",
    "            label,text = line.split(\"\\t\")\n",
    "            cleaned_text = clean_text(text)\n",
    "            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(\" \")]  # 把单词转换成词典中的位置\n",
    "            pad_list = pad(word_token_list,MAX_LEN)\n",
    "            out_line = label+\"\\t\"+\" \".join([str(x) for x in pad_list])\n",
    "            fout.write(out_line+\"\\n\")\n",
    "        \n",
    "text_to_token(train_data_path,train_token_path)\n",
    "text_to_token(test_data_path,test_token_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9150d1",
   "metadata": {},
   "source": [
    "接着将token文本按照样本分割，每个文件存放一个样本的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割样本\n",
    "import os\n",
    "\n",
    "if not os.path.exists(train_samples_path):\n",
    "    os.mkdir(train_samples_path)\n",
    "    \n",
    "if not os.path.exists(test_samples_path):\n",
    "    os.mkdir(test_samples_path)\n",
    "    \n",
    "def split_samples(token_path,samples_dir):\n",
    "    with open(token_path,\"r\",encoding = 'utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            with open(samples_dir+\"%d.txt\"%i,\"w\",encoding = \"utf-8\") as fout:\n",
    "                fout.write(line)\n",
    "            i = i+1\n",
    "\n",
    "split_samples(train_token_path,train_samples_path)\n",
    "split_samples(test_token_path,test_samples_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af454382",
   "metadata": {},
   "source": [
    "创建数据集Dataset，从文件名称列表中读取文件内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5366df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imdbDataset(Dataset):\n",
    "    def __init__(self,samples_dir):\n",
    "        self.samples_dir = samples_dir\n",
    "        self.samples_paths = os.listdir(samples_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_paths)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        path = self.samples_dir + self.samples_paths[index]\n",
    "        with open(path,\"r\",encoding = \"utf-8\") as f:\n",
    "            line = f.readline()\n",
    "            label,tokens = line.split(\"\\t\")\n",
    "            label = torch.tensor([float(label)],dtype = torch.float)\n",
    "            feature = torch.tensor([int(x) for x in tokens.split(\" \")],dtype = torch.long)\n",
    "            return  (feature,label)\n",
    "ds_train = imdbDataset(train_samples_path)\n",
    "ds_test = imdbDataset(test_samples_path)\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = True,num_workers=4)\n",
    "dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=4)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce201399",
   "metadata": {},
   "source": [
    "这里数据的样子：\n",
    "<img style=\"float: center;\" src=\"images/8.png\" width=\"70%\">\n",
    "\n",
    "features里每一行是一个句子，这里都转成了单词的索引表示，而label里的每一个就是句子对应的分类，好评和差评。\n",
    "\n",
    "这种方式比较麻烦的就是前面的预处理部分，把数据进行切割，然后构建词典，存到文件等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
