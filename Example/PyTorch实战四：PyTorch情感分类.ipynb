{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efcd579",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "\n",
    "主要用torchtext工具包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d8d93",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4597627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ca4ff",
   "metadata": {},
   "source": [
    "## 下载数据集\n",
    "\n",
    "下载IMDb数据集，分成train/test两个torchtext.datasets类别。\n",
    "\n",
    "数据用Fields处理：\n",
    "- TorchText中的Field决定你的数据会被怎样处理。\n",
    "- 情感分类中，所需要接触到的数据由文本字符串和两种情感（pos/neg）\n",
    "- Field的参数制定了数据会被如何处理。\n",
    "  - 使用TEXT field定义如何处理电影评论\n",
    "  - 使用LABEL field处理两个情感类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5101e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huris\\anaconda3\\envs\\sEMG\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c646a2",
   "metadata": {},
   "source": [
    "注意这里要安装一个spacy包，使用命令：\n",
    "- pip install spacy\n",
    "- python -m spacy download en\n",
    "\n",
    "如果第二条命令无法运行，报网络连接错误，可以参考下面的博客：\n",
    "- https://www.cnblogs.com/xiaolan-Lin/p/13286885.html\n",
    "\n",
    "注意博客中下载的en_core_web_sm和en_core_web_md版本需要与spacy版本一致：\n",
    "- https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb406634",
   "metadata": {},
   "source": [
    "TEXT fielf带有：tokenize='spacy'，表示用spaCy tokenizer来tokenize英文句子。\n",
    "\n",
    "如果不特别声明tokenize这个参数，则默认的分词方法是使用空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d0f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a21ad",
   "metadata": {},
   "source": [
    "IMDb数据集一共有50000条电影评论，每个评论都被标注为正面或负面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a80c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Bromwell',\n",
       "  'High',\n",
       "  'is',\n",
       "  'a',\n",
       "  'cartoon',\n",
       "  'comedy',\n",
       "  '.',\n",
       "  'It',\n",
       "  'ran',\n",
       "  'at',\n",
       "  'the',\n",
       "  'same',\n",
       "  'time',\n",
       "  'as',\n",
       "  'some',\n",
       "  'other',\n",
       "  'programs',\n",
       "  'about',\n",
       "  'school',\n",
       "  'life',\n",
       "  ',',\n",
       "  'such',\n",
       "  'as',\n",
       "  '\"',\n",
       "  'Teachers',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'My',\n",
       "  '35',\n",
       "  'years',\n",
       "  'in',\n",
       "  'the',\n",
       "  'teaching',\n",
       "  'profession',\n",
       "  'lead',\n",
       "  'me',\n",
       "  'to',\n",
       "  'believe',\n",
       "  'that',\n",
       "  'Bromwell',\n",
       "  'High',\n",
       "  \"'s\",\n",
       "  'satire',\n",
       "  'is',\n",
       "  'much',\n",
       "  'closer',\n",
       "  'to',\n",
       "  'reality',\n",
       "  'than',\n",
       "  'is',\n",
       "  '\"',\n",
       "  'Teachers',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'The',\n",
       "  'scramble',\n",
       "  'to',\n",
       "  'survive',\n",
       "  'financially',\n",
       "  ',',\n",
       "  'the',\n",
       "  'insightful',\n",
       "  'students',\n",
       "  'who',\n",
       "  'can',\n",
       "  'see',\n",
       "  'right',\n",
       "  'through',\n",
       "  'their',\n",
       "  'pathetic',\n",
       "  'teachers',\n",
       "  \"'\",\n",
       "  'pomp',\n",
       "  ',',\n",
       "  'the',\n",
       "  'pettiness',\n",
       "  'of',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'situation',\n",
       "  ',',\n",
       "  'all',\n",
       "  'remind',\n",
       "  'me',\n",
       "  'of',\n",
       "  'the',\n",
       "  'schools',\n",
       "  'I',\n",
       "  'knew',\n",
       "  'and',\n",
       "  'their',\n",
       "  'students',\n",
       "  '.',\n",
       "  'When',\n",
       "  'I',\n",
       "  'saw',\n",
       "  'the',\n",
       "  'episode',\n",
       "  'in',\n",
       "  'which',\n",
       "  'a',\n",
       "  'student',\n",
       "  'repeatedly',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'burn',\n",
       "  'down',\n",
       "  'the',\n",
       "  'school',\n",
       "  ',',\n",
       "  'I',\n",
       "  'immediately',\n",
       "  'recalled',\n",
       "  '.........',\n",
       "  'at',\n",
       "  '..........',\n",
       "  'High',\n",
       "  '.',\n",
       "  'A',\n",
       "  'classic',\n",
       "  'line',\n",
       "  ':',\n",
       "  'INSPECTOR',\n",
       "  ':',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'here',\n",
       "  'to',\n",
       "  'sack',\n",
       "  'one',\n",
       "  'of',\n",
       "  'your',\n",
       "  'teachers',\n",
       "  '.',\n",
       "  'STUDENT',\n",
       "  ':',\n",
       "  'Welcome',\n",
       "  'to',\n",
       "  'Bromwell',\n",
       "  'High',\n",
       "  '.',\n",
       "  'I',\n",
       "  'expect',\n",
       "  'that',\n",
       "  'many',\n",
       "  'adults',\n",
       "  'of',\n",
       "  'my',\n",
       "  'age',\n",
       "  'think',\n",
       "  'that',\n",
       "  'Bromwell',\n",
       "  'High',\n",
       "  'is',\n",
       "  'far',\n",
       "  'fetched',\n",
       "  '.',\n",
       "  'What',\n",
       "  'a',\n",
       "  'pity',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  \"n't\",\n",
       "  '!'],\n",
       " 'label': 'pos'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f712f3",
   "metadata": {},
   "source": [
    "## 划分数据集\n",
    "\n",
    "由于现在只有train/test两个分类，需要创建一个valid集，使用.split()创建新的分类。\n",
    "\n",
    "默认的数据分割是70|30，如果声明split_ratio，可以改变split之间的比例。（split_ratio=0.8表示80%是训练集，20%是验证集）\n",
    "\n",
    "如果声明random_state参数，可以确保每次分割的数据集都是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d183da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860a8af",
   "metadata": {},
   "source": [
    "## 创建词典\n",
    "\n",
    "vocabulary把每个单词——映射到一个数字。\n",
    "\n",
    "使用最常见的25k个单词来构建单词表，用max_size参数。\n",
    "\n",
    "其他所有单词都用\\<unk\\>表示。\n",
    "    \n",
    "这里采用一种预训练，用的都是预训练好的glove向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306f68ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")  # 25002\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62aa0a",
   "metadata": {},
   "source": [
    "创建完词典，直接用stoi(string to int)或者itos(int to string)查看单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531cb1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])  # ['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
    "print(LABEL.vocab.stoi)  # defaultdict(None, {'neg': 0, 'pos': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50397eb",
   "metadata": {},
   "source": [
    "把句子传进模型时，按照一个个batch传进去，即一次传入若干个句子，并且每个batch中的句子必须是相同的长度。\n",
    "\n",
    "为了确保句子长度相同，TorchText会把短的句子pad到和最长的句子等长。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18692b1",
   "metadata": {},
   "source": [
    "## 创建iterators\n",
    "\n",
    "每个iteration都会返回一个batch的examples，使用BucketIterator。\n",
    "\n",
    "BucketIterator会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。\n",
    "\n",
    "此处模型代码有一个问题，就是把\\<pad\\>也当作模型的输入进行训练，更好地做法是在模型中把\\<pad\\>产生的输出给清除掉。\n",
    "\n",
    "如果有GPU，还可以指定每个iteration返回的tensor都在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62edabde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acting',\n",
       " 'is',\n",
       " 'luck',\n",
       " ':',\n",
       " 'encouraged',\n",
       " 'makes',\n",
       " 'is',\n",
       " 'Cause',\n",
       " 'I',\n",
       " \"'s\",\n",
       " 'have',\n",
       " 'quite',\n",
       " 'stunning',\n",
       " 'Dix',\n",
       " 'find',\n",
       " 'br',\n",
       " 'finished',\n",
       " 'is',\n",
       " 'and',\n",
       " 'Lugosi',\n",
       " 'this',\n",
       " 'watched',\n",
       " 'fans',\n",
       " 'movie',\n",
       " 'started',\n",
       " 'best',\n",
       " 'really',\n",
       " 'find',\n",
       " 'Almighty',\n",
       " 'too',\n",
       " 'friend',\n",
       " 'is',\n",
       " 'watching',\n",
       " 'is',\n",
       " 'suspenseful',\n",
       " 'Girlfight',\n",
       " 'show',\n",
       " 'do',\n",
       " 'times',\n",
       " 'Michael',\n",
       " 'had',\n",
       " 'director',\n",
       " 'off',\n",
       " 'went',\n",
       " 'was',\n",
       " 'fans',\n",
       " ',',\n",
       " '<unk>',\n",
       " 'the',\n",
       " 'gently',\n",
       " 'I',\n",
       " 'I',\n",
       " 'rented',\n",
       " 'author',\n",
       " 'wonderful',\n",
       " 'this',\n",
       " 'is',\n",
       " 'love',\n",
       " 'enjoy',\n",
       " 'on',\n",
       " 'movie',\n",
       " 'all',\n",
       " 'absolutely',\n",
       " 'day']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Iterator的建立\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE,device=device)\n",
    "\n",
    "# 看一个样本长啥样，Iterator里，每个单词都用词典中的位置代替，每一列是一个样本\n",
    "[TEXT.vocab.itos[i] for i in next(iter(train_iterator)).text[1, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff826abc",
   "metadata": {},
   "source": [
    "# 模型建立与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8456533",
   "metadata": {},
   "source": [
    "## Word Averaging模型\n",
    "\n",
    "Word Averaging模型把每个单词都通过Embedding层投射成word embedding vector，然后把一句话中的所有word vector做一个平均，就是整个句子的vector表示，之后把这个句子vector传入一个Linear层，做分类即可。\n",
    "<img style=\"float: center;\" src=\"images/9.png\" width=\"70%\">\n",
    "\n",
    "网络的前向传播是这样的：\n",
    "- 首先输入text，即上面准备好的数据的其中一个Iterator，大小是[seq_len, batch_size]\n",
    "- 接下去通过一个embedding层，得到每个单词的embedding向量，矩阵大小[seq_len, batch_size, embed_dim]\n",
    "- 把前两个维度换一下，[batch_size, seq_len, embed_dim]，接下来就是在句子的维度进行求平均，即把句子长度那个维度压扁，此处采用的是avg_pool2d来做平均池化，avg_pool2d的kernel size是(embedded.shape[1], 1)，所以句子长度的那个维度会被压扁。\n",
    "- 经过这个操作后，矩阵的维度变成[batch_size, 1, embed_dim]，就是seq_len的维度上进行一个求平均，这样就得到了一句话的embedding。\n",
    "- 之后通过全连接层得到最后的输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b460eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # [sent len, batch size, emb dim]\n",
    "        embedded = embedded.permute(1, 0, 2) # [batch size, sent len, emb dim]\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) # [batch size, embedding_dim]\n",
    "        # 这个就相当于在seq_len维度上做了一个平均\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a53cec",
   "metadata": {},
   "source": [
    "**PyTorch搭建神经网络技巧：**\n",
    "\n",
    "先搭好大框架：\n",
    "```python\n",
    "class WordAVGModel(nn.Module):\n",
    "\tdef __init__(self, ):\n",
    "\t\tpass\n",
    "\tdef forward(self, text):\n",
    "\t\tpass\n",
    "\tdef .....\n",
    "```\n",
    "\n",
    "之后先写forward函数，这是网络的计算过程，然后再写初始化的部分，因为可能一上来不知道网络里面到底应该有哪些网络层，先写前向传播之后，就有利于知道需要定义哪些层，这时候再写init的时候就好写了，最后再定义\\_\\_init\\_\\_里的形参，考虑\\_\\_init\\_\\_的成员对象用到了哪些需要外界传递的参数。（总体而言就是自下而上地写）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e45d07",
   "metadata": {},
   "source": [
    "搭建完网络模型之后，就可以定义一些参数，然后建立这个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "555fad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fac28",
   "metadata": {},
   "source": [
    "网络训练部分，这个任务里给出一个embedding向量地初始化技巧，**采用已经训练好的embedding向量进行一个初始化（glove）**，这相当于网络训练的时候，embedding这部分只需要简单的微调就可以，大大加快了训练。\n",
    "\n",
    "因此这里直接把训练好的glove参数放到embedding层的w上：类似于一种迁移思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85ce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# 相当于把embedding向量的w参数初始化成了通过大量语句训练好的embedding， \n",
    "# 这样网络训练的时候向量只需微调\n",
    "\n",
    "# 两个特殊字符初始化为0\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17dfed7",
   "metadata": {},
   "source": [
    "开始训练，事先写好了训练和验证函数，好处是换了模型，训练的函数不用改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2692d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    " \n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d338e75",
   "metadata": {},
   "source": [
    "下面再加两个辅助函数，一个计算acc，一个计算运行时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27375130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    " \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85316d2",
   "metadata": {},
   "source": [
    "正式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006be2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()   # 这个是加了sigmoid的损失函数\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d24d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.684 | Train Acc: 58.54%\n",
      "\t Val. Loss: 0.617 |  Val. Acc: 71.31%\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.639 | Train Acc: 71.67%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 75.56%\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.567 | Train Acc: 78.58%\n",
      "\t Val. Loss: 0.456 |  Val. Acc: 79.69%\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.497 | Train Acc: 83.08%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 82.67%\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.438 | Train Acc: 86.10%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 84.79%\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.389 | Train Acc: 87.81%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 86.22%\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.350 | Train Acc: 89.06%\n",
      "\t Val. Loss: 0.408 |  Val. Acc: 87.10%\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.322 | Train Acc: 90.01%\n",
      "\t Val. Loss: 0.423 |  Val. Acc: 87.68%\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.295 | Train Acc: 90.92%\n",
      "\t Val. Loss: 0.442 |  Val. Acc: 87.94%\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.274 | Train Acc: 91.51%\n",
      "\t Val. Loss: 0.458 |  Val. Acc: 88.51%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'wordavg-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cff257",
   "metadata": {},
   "source": [
    "对模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013f8310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.433 | Test Acc: 85.50%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('wordavg-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c88035",
   "metadata": {},
   "source": [
    "实际测一下效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5608d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.31305413395711e-10, 1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n",
    "\n",
    "predict_sentiment(\"This film is terrible\"), predict_sentiment(\"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bf68c",
   "metadata": {},
   "source": [
    "# RNN Model\n",
    "\n",
    "尝试把模型换成一个recurrent neural network（RNN）\n",
    "\n",
    "RNN经常用于encode一个sequence：$h_t=RNN(x_t, h_{t-1})$\n",
    "- 使用最后一个hidden state$h_T$来表示整个句子\n",
    "- 把$h_T$通过一个线性变换$f$，然后用于预测句子的情感\n",
    "\n",
    "使用一个2层双向LSTM网络：\n",
    "<img style=\"float: center;\" src=\"images/10.png\" width=\"70%\">\n",
    "\n",
    "网络计算过程：\n",
    "- 输入一批句子，大小为[seq_len, batch_size]\n",
    "- 经过一个embedding层得到每个单词的embedding向量，此时维度[seq_len, batch_size, embed_dim]\n",
    "- 经过一个双向LSTM，并且是2层堆叠起来的，此时网络输出[seq_len, batch_size, hidden_size\\*num_directions]\n",
    "- LSTM的隐藏状态h和c是[num_layers\\*num_directions, batch_size, hidden_size]，此时需要拿到最后一层最后一个时间步LSTM的隐藏层状态，对他俩进行一个拼接\n",
    "- 最后通过全连接层得到结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aa76eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text)) #[sent len, batch size, emb dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "    \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) # [batch size, hid dim * num directions]\n",
    "        \n",
    "        #and apply dropout\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54611c32",
   "metadata": {},
   "source": [
    "前向传播时，标注一下每一层之后变量的维度变化。（一个好习惯）\n",
    "\n",
    "之后是训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739199d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()   # 这个是加了sigmoid的损失函数\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e01ca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.681 | Train Acc: 55.56%\n",
      "\t Val. Loss: 0.645 |  Val. Acc: 63.93%\n",
      "Epoch: 02 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.641 | Train Acc: 63.97%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 70.76%\n",
      "Epoch: 03 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.679 | Train Acc: 56.51%\n",
      "\t Val. Loss: 0.650 |  Val. Acc: 61.66%\n",
      "Epoch: 04 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.603 | Train Acc: 67.76%\n",
      "\t Val. Loss: 0.604 |  Val. Acc: 69.84%\n",
      "Epoch: 05 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.565 | Train Acc: 71.22%\n",
      "\t Val. Loss: 0.543 |  Val. Acc: 76.31%\n",
      "Epoch: 06 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.498 | Train Acc: 76.70%\n",
      "\t Val. Loss: 0.501 |  Val. Acc: 77.04%\n",
      "Epoch: 07 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.448 | Train Acc: 79.46%\n",
      "\t Val. Loss: 0.414 |  Val. Acc: 81.41%\n",
      "Epoch: 08 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.438 | Train Acc: 80.36%\n",
      "\t Val. Loss: 0.468 |  Val. Acc: 78.95%\n",
      "Epoch: 09 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.507 | Train Acc: 76.17%\n",
      "\t Val. Loss: 0.471 |  Val. Acc: 77.41%\n",
      "Epoch: 10 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.622 | Train Acc: 67.44%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 66.30%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a6be7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.432 | Test Acc: 80.75%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('lstm-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd4dbc",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "\n",
    "CNN也可以处理语言序列。\n",
    "\n",
    "卷积神经网络的计算过程相比于前面两个网络模型有点难理解，因此CNN一般用于处理图像，而语言模型是时序数据，CNN如何捕捉时序序列之间的关联呢？\n",
    "\n",
    "此处利用卷积+池化的操作，不过这次用的卷积核不是正方形的，其网络结构来自于：\n",
    "- 《Convolutional Neural Networks for Setence Classification》\n",
    "<img style=\"float: center;\" src=\"images/11.png\" width=\"70%\">\n",
    "\n",
    "前向传播的过程：\n",
    "- 首先接收一批句子，维度是[batch_size, seq_len]，注意这里先转换了维度\n",
    "- 之后经过embedding层，得到[batch_size, seq_len, emb_dim]\n",
    "- 2维卷积接收的输入是4维的，因为要把这个东西看成一个图像才能进行二维卷积\n",
    "  - 2维卷积层接收的输入是[batch_size, in_chnnels, Height, Width]\n",
    "  - 而embedding之后是3维的，因此需要在第2个维度扩扩出一个1来（表示通道数是1维）\n",
    "- 后面的seq_len和emb_dim表示图像的长和宽，之后在这里做卷积\n",
    "- 卷积核大小[filter_size, emb_dim]，这样对[seq_len, emb_dim]进行卷积后，就会把emb_dim这个维度变成1，从而提取了每个单词之间的特征\n",
    "- 这时候输出维度是[batch_size, num_filters, seq_len-filter_size+1, 1]\n",
    "  - 这次卷积就是在embedding向量之后的后2维上进行卷积提取单词之间的特征\n",
    "  - 无非就是卷积核的第2个维度是emb_dim，卷积的时候把每个单词全部的词向量都进行了运算，因为考虑了全部的词向量后才是一个完整的单词\n",
    "  - 第二个维度是过滤器的个数。\n",
    "- 得到了上面这个向量，把最后一个维度去掉，然后在seq_len-filter_size+1这个维度上进行最大池化，这个表示新的单词与单词间的那种关系，得到一个[batch_size, num_filters, 1]的张量，然后把第二维度去掉，再进行全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21663935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_filters, filter_size, out_size, dropout, pad_idx):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(filter_size, embedding_size))\n",
    "        self.linear = nn.Linear(num_filters, out_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0)     # [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)      # [batch_size, seq_len, emb_dim]\n",
    "        embedded = embedded.unsqueeze(1)    # [batch_size, 1, seq_len, emb_dim]\n",
    "        conved = F.relu(self.conv(embedded))   # [batch_size, num_filters, seq_len-filter_size+1]\n",
    "        conved = conved.squeeze(3)\n",
    "        pooled = F.max_pool1d(conved, conved.shape[2])   # 把第二个维度压扁， [batch_size, numf, 1]\n",
    "        pooled = pooled.squeeze(2)    # [batch_size, num_filters]\n",
    "        pooled = self.dropout(pooled)   # [batch_size, num_filters]\n",
    "        \n",
    "        return self.linear(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2061d56",
   "metadata": {},
   "source": [
    "CNN网络的强大在于可以使用多种卷积核，进行多层卷积操作，这时候可以把这些卷积层写道一个ModuleList里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0be6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0)   # [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embed_dim]\n",
    "        embedded = embedded.unsqueeze(1)   # [batch_size, 1, seq_len, embed_dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conv_n: [batch_size, num_filters, seq_len-fliter[n]+1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled_n: [batch_size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # [batch_size, n_filters*len(filter_size)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4548a",
   "metadata": {},
   "source": [
    "建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c1b54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "OUTPUT_DIM = 1\n",
    "#FILTER_SIZE = 3\n",
    "FILTER_SIZE = [3, 4, 5]   # 使用多层卷积， 每一层用不同的卷积核\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN_Model(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZE, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53546ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()   # 这个是加了sigmoid的损失函数\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "[TEXT.vocab.itos[i] for i in next(iter(train_iterator)).text[1, :]]\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'CNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b995504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('CNN-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd623e4f",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "- PyTorch搭建神经网络，先搭框架，从下往上展开，从forward到init, 再到init的参数\n",
    "- 神经网络写前向传播的时候，养成核对张量维度的习惯\n",
    "- 神经网络训练的时候，可以把train和evaluate写成函数的方式，之后换模型也不用改变训练代码\n",
    "- 保存最好的模型，打印日志信息观察模型是否正常工作\n",
    "\n",
    "- 关于情感分类时训练模型的技巧，embedding的初始化参数可以用已经训练好的词向量，在这个基础上进行微调\n",
    "- 情感分类或者语言模型，有个torchtext包可以准备数据，数据初始化或者是构建成Iterator。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
