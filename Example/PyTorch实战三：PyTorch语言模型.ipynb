{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b15f05c",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "目标：给定一个句子，模型输出这句话出现的概率是多少，这样的话可以做很多任务，比如给定一句话进行填词，文本的生成等。\n",
    "\n",
    "标准定义：给定语言序列$w_1,w_2,...,w_n$，语言模型就是计算该序列的概率，由链式法则：\n",
    "\n",
    "$P(w_1,w_2,...,w_n)=P(w_1)P(w_2|w_1)\\cdots P(w_n|w_1,...,w_{n-1})$\n",
    "\n",
    "统计语言模型中，采用极大似然估计来计算每个词出现的条件概率，但对于任意长的自然语言语句，这个方法可能行不通，于是后来就引入马尔科夫假设，即当前次出现的概率只依赖于前n-1个词，于是得到如下公式：\n",
    "\n",
    "$P(w_i|w_1,w_2,...,w_{i-1})=P(w_i|w_{i-n+1},...,w_{i-1})$\n",
    "\n",
    "于是最终结论为：\n",
    "\n",
    "$P(w_1,w_2,...,w_m)=\\prod^m_{i=1}P(w_i|w_1,w_2,...,w_{i-1})\\approx \\prod^m_{i=1}P(w_i|w_{i-n+1},...,w_{i-1})$\n",
    "\n",
    "本次任务的目的就是通过循环神经网络模型来计算这个概率，相比较单纯的前馈神经网络，隐状态的传递性使得RNN语言模型原则上可以捕捉前向序列的所有信息（虽然可能比较弱）。\n",
    "\n",
    "通过在整个训练集上优化交叉熵来训练模型，使得网络能够尽可能建模出自然语言序列与后续词之间的内在联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20402e",
   "metadata": {},
   "source": [
    "本文主要实现这样一个模型：\n",
    "<img style=\"float: center;\" src=\"images/7.png\" width=\"70%\">\n",
    "\n",
    "模型的输入是一个句子，模型的输出也是一个句子。\n",
    "- 比如\"I like China\"\n",
    "\n",
    "在第一个时间步的时候输入\"I\"，然后通过一个embedding层得到\"I\"的词向量表示，通过LSTM网络得到当前时间步的稳态值，经过一个全连接网络得到\"like\"，然后\"like\"作为第二个时间步的输入，得到第二个时间步的输出\"China\"，这样一步步地进行传递，进行模型训练。\n",
    "\n",
    "当下一次输出\"I\"的时候，模型可能就会继续输出\"like China\"，这样就能得到我们想要的句子，使得模型的输出更加看起来像\"人话\"，相当于$P(I like China)>P(like China I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceafbff",
   "metadata": {},
   "source": [
    "# 实现一个RNN语言模型\n",
    "\n",
    "依然用之前的text8，里面分成了训练集，交叉验证集和测试集，每个数据集里是一段文字，每个单词之间用空格进行隔开。\n",
    "\n",
    "处理的方式依然是拿到这些数据集，然后构建词典，数据迭代器，index_to_word，word_to_index这些东西，不过这次用torchtext这个工具包完成任务。\n",
    "\n",
    "其次就是语言模型的定义，这里会搭建一个模型，然后进行模型的训练，使用的损失函数为交叉熵损失，因为每一个时间步其实在做一个多分类的问题，所以这个损失函数刚刚是解决这样的一个任务，训练过程就是前向传播，计算损失，反向传播，梯度更新，梯度清零，模型保存等。\n",
    "\n",
    "这里也会涉及语言模型的一些训练细节，比如隐藏状态的batch之间的传递，梯度修剪，动态更新学习率等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7a31d",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32472639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "    \n",
    "## 定义常用的参数\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 64\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "NET_LAYER_SIZE = 2\n",
    "\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41657c5",
   "metadata": {},
   "source": [
    "## 构建词典和迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6d3172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field决定了数据如何被处理，这个例子中先转换成小写\n",
    "TEXT = torchtext.legacy.data.Field(lower=True)\n",
    "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path='./data/', \n",
    "                    train='text8.train', validation='text8.dev', test='text8.test', text_field=TEXT)\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "len(TEXT.vocab)  # 那两个是两个外界符号   30002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959cdac",
   "metadata": {},
   "source": [
    "构建词典基本上是用torchtext包完成：\n",
    "- torchtext的一个重要概念是Field，它决定数据会被如何处理。使用TEXT的Field来处理文本数据，lower=True将所有的单词都变成小写。\n",
    "- torchtext提供LanguageModelingDataset类来帮助处理语言模型数据集。\n",
    "- build_vocab根据提供的训练数据集来创建最高频单词的单词表，max_size帮助限顶单词总量\n",
    "\n",
    "根据上一步，不仅构造好了词典，同时也制作好了index_to_word和word_to_index两种映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37459d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]   # index to string   index to word\n",
    "TEXT.vocab.stoi['apple']    # word to index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e186180",
   "metadata": {},
   "source": [
    "获取训练集，验证集和测试集的iter，BPTTIterator可以连续地得到连贯地句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20d1ecc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.legacy.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=device, \n",
    "    bptt_len=50, repeat=False, shuffle=True)\n",
    "# 这里是50个句子为一句，下面的text和target都是50*32，50代表单词的个数，32代表batch的大小\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b224756",
   "metadata": {},
   "source": [
    "查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4a1f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility\n",
      "in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 1].data]))\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:, 1].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65b2f5",
   "metadata": {},
   "source": [
    "模型输入是一串数字，输出也是一串数字，它们之间相差一个位置。\n",
    "\n",
    "语言模型的目标是根据之前的单词预测下一个单词，如果不明显可以多看几个batch："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d3560d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "reject that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is\n",
      "\n",
      "that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is quite\n",
      "1\n",
      "quite different from japanese culture never <unk> after a certain age the men had full <unk> and <unk> men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it <unk> behind the women <unk> their mouths arms <unk> and sometimes their\n",
      "\n",
      "different from japanese culture never <unk> after a certain age the men had full <unk> and <unk> men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it <unk> behind the women <unk> their mouths arms <unk> and sometimes their <unk>\n",
      "2\n",
      "<unk> starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round\n",
      "\n",
      "starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round the\n",
      "3\n",
      "the body and is tied with a girdle of the same material women also wear an <unk> of japanese cloth in winter the skins of animals were worn with <unk> of <unk> and boots made from the skin of dogs or salmon both sexes are fond of <unk> which are\n",
      "\n",
      "body and is tied with a girdle of the same material women also wear an <unk> of japanese cloth in winter the skins of animals were worn with <unk> of <unk> and boots made from the skin of dogs or salmon both sexes are fond of <unk> which are said\n",
      "4\n",
      "said to have been made of <unk> in former times as also are bead <unk> called <unk> which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate\n",
      "\n",
      "to have been made of <unk> in former times as also are bead <unk> called <unk> which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate raw\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    batch = next(it)\n",
    "    print(k)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))\n",
    "    print()\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:, 2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa73ef",
   "metadata": {},
   "source": [
    "可以发现每个batch之间的句子是连起来的，下一个batch的开头正好是上一个batch的结尾。\n",
    "<img style=\"float: center;\" src=\"images/8.png\" width=\"70%\">\n",
    "\n",
    "训练时，会采用一些技巧进行batch与batch之间隐藏状态的传递。\n",
    "\n",
    "注意，如果不是训练语言模型，一般batch之间是没有啥关系的。\n",
    "\n",
    "因此这个技巧一般只适合语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d4688",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "继承nn.Module，然后初始化函数，前向传播。\n",
    "\n",
    "以上三个是必备的，其他根据需要自定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8436cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"一个简单的循环神经网络, \n",
    "       主要包括词嵌入层，\n",
    "       一个循环神经网络，\n",
    "       一个线性层，\n",
    "       一个Dropout层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        rnn_type: 表示rnn的类型，有RNN，LSTM，GRU\n",
    "        ntoken: 词典内的单词个数\n",
    "        ninp: 这个是输入维度， 也是embedding维度\n",
    "        nhid: 这个是神经网络隐藏单元的个数\n",
    "        nlayers: 这个是神经网络的层数\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # dropout层要放在最前面\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = RNN(ninp, nhid, nlayers, nonlinearity='relu', dropout=dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "    \n",
    "    # 权重初始化\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)   # 下划线代表原位操作\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    # forward\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "         这个过程就是先embedding，然后经过LSTM，然后通过一个线性层转换输出单词表\n",
    "         input: 指的是当前时间步的输入\n",
    "         hidden：指的是前一隐藏状态\n",
    "        \"\"\"\n",
    "        # input: (seq_len, batch)     \n",
    "        # torch的LSTM的默认输入是第一个维度是seq_len，第二个维度是batch\n",
    "        # 那里指定batch_first=True\n",
    "        embed = self.drop(self.embed(input))      # (seq_len, batch, embedding_size)\n",
    "        output, hidden = self.rnn(embed, hidden)  \n",
    "        # output: (seq_len, batch, hidden_size)   \n",
    "        # hidden: [(1, batch_size, hidden_size), (1, batch_size, hidden_size)]  一个h， 一个c 对于LSTM\n",
    "        output = self.drop(output)\n",
    "        linear = self.linear(output.view(-1, output.size(2)))   # [seq_len*batch, vocab_size]\n",
    "        return linear.view(output.size(0), output.size(1), linear.size(1)), hidden   \n",
    "       # 这个地方之所以要把hidden也返回， 是因为hidden表示batch的最后一个时间步的隐藏状态信息， 而通过之前我们发现， 语言模型里面\n",
    "        # batch与batch之间是相联系的， 上一个batch的最后一个单词正好是下一个batch的第一个单词， 那么就像最后一个单词的batch保存下来\n",
    "        # 供下面的batch使用， 使得这个hidden在batch之间进行传递\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, bsz, requires_grad=True):    \n",
    "        # 初始化隐藏层状态有个技巧了  隐藏层状态的大小是(nlayers, batch_size, hidden_size), 但是我们并不知道此时的参数是在cuda还是\n",
    "        # 在CPU上， 所以下面一个代码是获得某一次batch时的权重参数， 通过这个我们初始化hidden， 这样就可以保证类型一样了\n",
    "        weight = next(self.parameters())   # 获取某个iter时的参数\n",
    "        if self.rnn_type == 'LSTM':       # 因为LSTM的时候是有c和h的\n",
    "            return (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),  # 创建0张量， 且和weight是同一类型\n",
    "                   weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))\n",
    "        else:\n",
    "            return weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0178ee6",
   "metadata": {},
   "source": [
    "建立模型，参数包括词典中单词的个数，embedding的维度，神经网络隐藏单元的个数，网络的层数，这些在具体运算中确定了参数的维度信息。\n",
    "\n",
    "在初始化隐藏状态信息的时候用到了一个技巧，就是当不知道参数在GPU还是CPU上，所以那个地方是获取了目前模型的参数，通过这个参数建立张量，这样就保证了新建立的张量和模型的参数类型一致，这样就解决了是不是GPU或者CPU的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd5bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个模型\n",
    "model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, NET_LAYER_SIZE, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bf288",
   "metadata": {},
   "source": [
    "## 模型训练与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91930275",
   "metadata": {},
   "source": [
    "定义一个函数，获取隐藏层状态。\n",
    "\n",
    "因为通过前面的数据发现，每一个batch之间是有联系的，上一个batch的最后一句正好是下一个batch的第一句话，这样其实batch之间的隐藏状态也往下传递。\n",
    "\n",
    "但是这样面临的一个问题就是如果句子非常长，很容易梯度消失，所以需要只获取隐藏状态的值，而不需要整个计算图。\n",
    "\n",
    "这个函数就是单纯地拿出隐藏状态地值，而脱离了计算图，其涉及到PyTorch地hook机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "022405c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要先定义一个function，帮助我们把一个hidden state和计算图之前的历史分离\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()    # 这是钩子函数里面的一个机制， 从计算图里面摘下来\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)  # 这个应该是h和c同在的时候， 就需要分开来进行摘下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00b829",
   "metadata": {},
   "source": [
    "定义模型的损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd0b69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9df71e",
   "metadata": {},
   "source": [
    "训练之前，定义一个验证函数，用于保存效果比较好地模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "156b7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先定义模型评估的代码，模型评估的代码和模型的训练逻辑基本相同\n",
    "# 唯一的区别就是不需要反向传播，只需要正向传播\n",
    "def evaluate(model, data):\n",
    "    model.eval()   #  这个声明一下模型的状态\n",
    "    total_loss = 0.\n",
    "    it = iter(data)\n",
    "    total_count = 0.\n",
    "    with torch.no_grad():   # 注意，验证的时候就不需要求梯度了\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False) # 初始化隐藏状态\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target       # 获取X, Y\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            hidden = repackage_hidden(hidden)   # 只要隐藏状态\n",
    "            with torch.no_grad():\n",
    "                output, hidden = model(data, hidden)      # 前向传播\n",
    "\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))   # 计算损失\n",
    "            total_count += np.multiply(*data.size())    # np.multiply后面这个其实是data的两个shape相乘， *表示可变参数,算的是单词总个数\n",
    "            total_loss += loss.item() * np.multiply(*data.size())\n",
    "    \n",
    "    loss = total_loss / total_count\n",
    "    model.train() # 验证完了之后，别忘了恢复模型的训练状态\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5a694",
   "metadata": {},
   "source": [
    "模型训练思路：\n",
    "- 模型训练需要若干个epoch\n",
    "- 每个epoch，先获取数据的迭代器，然后分成多个batch\n",
    "- 每个batch的输入和输出都包装成cuda tensor\n",
    "- 前向传播，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算交叉熵损失\n",
    "- 清空模型当前的梯度\n",
    "- 反向传播\n",
    "- 梯度裁剪，防止梯度爆炸\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f3de5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 10.3095064163208\n",
      "best model, val loss:  10.30363225273546\n",
      "epoch 0 iter 1000 loss 6.710871696472168\n",
      "epoch 0 iter 2000 loss 6.775953769683838\n",
      "epoch 0 iter 3000 loss 6.623823165893555\n",
      "epoch 0 iter 4000 loss 6.368831634521484\n",
      "epoch 0 iter 5000 loss 6.605158805847168\n",
      "epoch 0 iter 6000 loss 6.412139892578125\n",
      "epoch 0 iter 7000 loss 6.299011707305908\n",
      "epoch 0 iter 8000 loss 6.321042537689209\n",
      "epoch 0 iter 9000 loss 6.231029033660889\n",
      "epoch 1 iter 0 loss 6.3042311668396\n",
      "best model, val loss:  6.021427464723173\n",
      "epoch 1 iter 1000 loss 6.043297290802002\n",
      "epoch 1 iter 2000 loss 6.233555316925049\n",
      "epoch 1 iter 3000 loss 6.2158284187316895\n",
      "epoch 1 iter 4000 loss 5.957834243774414\n",
      "epoch 1 iter 5000 loss 6.3020782470703125\n",
      "epoch 1 iter 6000 loss 6.099340915679932\n",
      "epoch 1 iter 7000 loss 6.138329029083252\n",
      "epoch 1 iter 8000 loss 6.104619026184082\n",
      "epoch 1 iter 9000 loss 6.057351112365723\n"
     ]
    }
   ],
   "source": [
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    it = iter(train_iter)      # 获得数据的迭代器\n",
    "    hidden = model.init_hidden(BATCH_SIZE)   # 初始化hidden为0\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()     # 包装成cuda\n",
    "        hidden = repackage_hidden(hidden)   # 这一步是为了把隐藏状态从计算图中取下来\n",
    "        \n",
    "        #梯度清零\n",
    "        model.zero_grad()\n",
    "        # forward pass \n",
    "        output, hidden = model(data, hidden)\n",
    "        # loss\n",
    "        #print(output.view(-1, VOCAB_SIZE), target.view(-1))  # 前者是一个[1600, 50002] 后者是一个1600\n",
    "        # 我猜是这么算的，后者里面是正确单词的位置，那么对应前面每一行相应的那个位置的数， \n",
    "        # 那么就用e^loc[target[i]] / e^out_put[i][j] j从1到50002 然后-log， 这样就会得出每一行的结果， 相当于依据\n",
    "        # 后面的target做了一个softmax， 然后取了-log， 最后再把所有行的那个结果取了一个平均\n",
    "\n",
    "        # 前者是一个(1600,30002)的矩阵，后者是一个(1600，)的向量\n",
    "        # 具体计算的时候先通过后面的target里的位置获取每一行相应位置的数值\n",
    "        # 然后通过softmax得到每一行里目标单词的概率，然后log一下取负号就得到每一行的交叉熵损失\n",
    "        # 最后再求个平均得到了最终的loss\n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))   \n",
    "        # backward \n",
    "        loss.backward()\n",
    "        # 梯度修剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "        \n",
    "        # 模型评估和保存\n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            # 模型保存时保存当前验证最小的模型，保存参数字典的方式进行保存\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print('best model, val loss: ', val_loss)\n",
    "                torch.save(model.state_dict(), 'lm-best.th')\n",
    "            else:\n",
    "                # 学习率衰减\n",
    "                scheduler.step()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba10ef0",
   "metadata": {},
   "source": [
    "导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d48c9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入最好的模型\n",
    "best_model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, NET_LAYER_SIZE, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    best_model = best_model.cuda()\n",
    "best_model.load_state_dict(torch.load('lm-best.th'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063b3ff",
   "metadata": {},
   "source": [
    "进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fc76906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 412.166529180177\n",
      "perplexity 412.166529180177\n"
     ]
    }
   ],
   "source": [
    "# 使用最好的模型在valid数据上计算perpleity\n",
    "val_loss = evaluate(best_model, val_iter)\n",
    "print('perplexity', np.exp(val_loss))\n",
    "\n",
    "# 使用最好的模型在test数据上计算perplexity\n",
    "test_loss = evaluate(best_model, test_iter)\n",
    "print('perplexity', np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288059f0",
   "metadata": {},
   "source": [
    "使用训练好的模型生成一些句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "227cf655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally hampshire sparked its ordinary beer lawful christians is and grew all one nine zero pc theft on <unk> in their a past while or assuming to the ideology food the core to be busy each element beginning while six zero watch <unk> a environment of lightning in poem can have vowel consumption by multiplications a potential is to organic own philosophical though day however travelling has a new reasoning cable eclipses revenue makes when ai the root was computed by <unk> recognizes bright pulsar in language the other engines quit chooses offline or a unrestricted district has still t\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的模型来生成一些句子\n",
    "hidden = best_model.init_hidden(1)   # batch_size大小为1\n",
    "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)   # 在0-30002中随机的产生一个单词， 作为第一时间步的输入\n",
    "words = []\n",
    "for i in range(100):\n",
    "    output, hidden = best_model(input, hidden)   # 前向传播\n",
    "    word_weights = output.squeeze().exp().cpu()  # 计算权重\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]   # 根据权重进行采样\n",
    "    input.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]     # 得到单词\n",
    "    words.append(word)\n",
    "print(\" \".join(words))   # 单词拼接成句子"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
