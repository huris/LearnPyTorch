{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2518ca59",
   "metadata": {},
   "source": [
    "# 手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ed6f4",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17524b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b9704",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "导入数据，并且构成DataLoader的数据格式，首先导入数据，由于手写数字识别的数据集在datasets包里，可以直接下载使用。\n",
    "\n",
    "下载并且完成一些基本的数据格式转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8380385b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = datasets.MNIST('data/', train=True, download=True, \n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor()      # 把图片转换成张量\n",
    "                            ])\n",
    "                           )\n",
    "mnist_data   # 这是一个Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951d2fa",
   "metadata": {},
   "source": [
    "下载完毕后，查看图片形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fcdfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minist_data[i]表示第几张图片，[i][0]表示图片的张量，[i][1]表示类别\n",
    "mnist_data[10][0].shape\n",
    "# 第一个维度是通道数，第二个是图片的长，第二个维度是图片的宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91241a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13066062, 0.30810776)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面求一下数据的均值和方差\n",
    "# 为了后面的数据标准化，更容易进行模型的训练\n",
    "data = [d[0].data.cpu().numpy() for d in mnist_data]\n",
    "np.mean(data), np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429037c0",
   "metadata": {},
   "source": [
    "有了Dataset，就可以构建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2265b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"data/\", train=True, download=True, \n",
    "                   transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.13066062,), (0.30810776,))   # 把所有数据进行归一化之后， 可以使得训练速度加快\n",
    "                   ])), batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True   # 这个也是为了加速训练\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"data/\", train=False, download=True, \n",
    "                  transform = transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.13066062,), (0.30810776,))\n",
    "                  ])), batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a8399",
   "metadata": {},
   "source": [
    "## 模型的构建与训练\n",
    "\n",
    "搭建模型（类似于LeNet结构）：卷积->池化->卷积->池化->两个全连接层\n",
    "\n",
    "卷积神经网络的搭建，有个比较重要的点就是每一步要弄清数据的维度变化。\n",
    "- 首先有一批图片，维度是[batch_size, 1, 28, 28]，PyTorch中，图片通道数位于第二维\n",
    "- 经过卷积层，卷积核个数20，大小5\\*5，数据维度[batch_size, 20, 24, 24]\n",
    "  - 这个20是卷积核个数，也是输出数据的通道数\n",
    "  - 长宽变化：(原尺寸-卷积核大小)/步长+1=(28-5)/1+1=24\n",
    "- 经过池化层，核大小2\\*2，图片长宽变为输入的一半，数据维度[batch_size, 20, 12, 12]\n",
    "- 经过卷积层，卷积核个数50，大小5\\*5，数据维度[batch, 50, 8, 8]\n",
    "- 经过池化层，核大小2\\*2，图片长宽变为输入的一半，数据维度[batch_size, 20, 4, 4]\n",
    "- 改变形状，数据维度[batch_size, 4\\*4\\*50]\n",
    "- 接两个全连接层，数据维度[batch_size, 10]\n",
    "\n",
    "最后变成一个10分类问题、"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efac12",
   "metadata": {},
   "source": [
    "Conv2d参数：\n",
    "- torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=‘zeros’)\n",
    "\n",
    "MaxPool2d参数：\n",
    "- torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9cc2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)   #  24*24   输出维度是20\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)   # 20*20   输出维度是50\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # X: 1*28*28\n",
    "        x = F.relu(self.conv1(x))     #  [batch_size,20, 24, 24]   第二维是out_channels\n",
    "        x = F.max_pool2d(x, 2, 2)    # [batch_size, 20, 12, 12]\n",
    "        x = F.relu(self.conv2(x))    # [batch_size, 50, 8, 8]\n",
    "        x = F.max_pool2d(x, 2, 2)   # [batch_size, 50, 4, 4]\n",
    "        x = x.view(-1, 4*4*50)      # [batch_size, 1, 4*4*50]\n",
    "        x = F.relu(self.fc1(x))      # [batch_size, 500]\n",
    "        x = self.fc2(x)  # [batch_size, 10]\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)     # 在第一个维度上进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719fa5a",
   "metadata": {},
   "source": [
    "网络构建完成后，定义函数进行训练\n",
    "\n",
    "依然是训练代码与测试代码分开写，这样训练的时候比较清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6924265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        pred = model(data)     # batch_size*10\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        \n",
    "        # SGD \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print('Train Epoch: {}, iteration: {}, Loss: {}'.format(epoch, idx, loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            total_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    total_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset) * 100.\n",
    "    print('Test loss: {}, Accuracy:{}'.format(total_loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f71f4",
   "metadata": {},
   "source": [
    "这里损失函数采用NLLLoss（就是softmax取负号），希望softmax之后，正确标签的概率越大越好，然后取负号就是希望正确标签的值越小越好，就对应NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99312501",
   "metadata": {},
   "source": [
    "开始模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae4e4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, Loss: 2.318403959274292\n",
      "Train Epoch: 0, iteration: 100, Loss: 0.5317106246948242\n",
      "Train Epoch: 0, iteration: 200, Loss: 0.49998578429222107\n",
      "Train Epoch: 0, iteration: 300, Loss: 0.21852730214595795\n",
      "Train Epoch: 0, iteration: 400, Loss: 0.12791477143764496\n",
      "Train Epoch: 0, iteration: 500, Loss: 0.38944199681282043\n",
      "Train Epoch: 0, iteration: 600, Loss: 0.14336960017681122\n",
      "Train Epoch: 0, iteration: 700, Loss: 0.1180097684264183\n",
      "Train Epoch: 0, iteration: 800, Loss: 0.3231854736804962\n",
      "Train Epoch: 0, iteration: 900, Loss: 0.029980473220348358\n",
      "Train Epoch: 0, iteration: 1000, Loss: 0.1075289249420166\n",
      "Train Epoch: 0, iteration: 1100, Loss: 0.05399481952190399\n",
      "Train Epoch: 0, iteration: 1200, Loss: 0.09147675335407257\n",
      "Train Epoch: 0, iteration: 1300, Loss: 0.17822223901748657\n",
      "Train Epoch: 0, iteration: 1400, Loss: 0.12545539438724518\n",
      "Train Epoch: 0, iteration: 1500, Loss: 0.19752934575080872\n",
      "Train Epoch: 0, iteration: 1600, Loss: 0.05556933954358101\n",
      "Train Epoch: 0, iteration: 1700, Loss: 0.020205724984407425\n",
      "Train Epoch: 0, iteration: 1800, Loss: 0.1368691474199295\n",
      "Test loss: 0.08219419644773006, Accuracy:97.46000000000001\n",
      "Train Epoch: 1, iteration: 0, Loss: 0.027313396334648132\n",
      "Train Epoch: 1, iteration: 100, Loss: 0.06486520171165466\n",
      "Train Epoch: 1, iteration: 200, Loss: 0.006381205283105373\n",
      "Train Epoch: 1, iteration: 300, Loss: 0.005362277384847403\n",
      "Train Epoch: 1, iteration: 400, Loss: 0.007097302470356226\n",
      "Train Epoch: 1, iteration: 500, Loss: 0.36157524585723877\n",
      "Train Epoch: 1, iteration: 600, Loss: 0.08100483566522598\n",
      "Train Epoch: 1, iteration: 700, Loss: 0.020946122705936432\n",
      "Train Epoch: 1, iteration: 800, Loss: 0.06745744496583939\n",
      "Train Epoch: 1, iteration: 900, Loss: 0.018566470593214035\n",
      "Train Epoch: 1, iteration: 1000, Loss: 0.05774923041462898\n",
      "Train Epoch: 1, iteration: 1100, Loss: 0.020621055737137794\n",
      "Train Epoch: 1, iteration: 1200, Loss: 0.010461883619427681\n",
      "Train Epoch: 1, iteration: 1300, Loss: 0.017339114099740982\n",
      "Train Epoch: 1, iteration: 1400, Loss: 0.167106032371521\n",
      "Train Epoch: 1, iteration: 1500, Loss: 0.004037347622215748\n",
      "Train Epoch: 1, iteration: 1600, Loss: 0.0864226296544075\n",
      "Train Epoch: 1, iteration: 1700, Loss: 0.0034394424874335527\n",
      "Train Epoch: 1, iteration: 1800, Loss: 0.10694953054189682\n",
      "Test loss: 0.04298409485127777, Accuracy:98.72\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca2d38",
   "metadata": {},
   "source": [
    "# CNN模型的迁移学习\n",
    "\n",
    "常用以下两种方式做迁移学习：\n",
    "- fine tuning：从一个预训练开始，改变一些模型的架构，然后继续调整模整个模型的参数。\n",
    "- feature extraction：不再改变预训练模型的参数，只更新改变过的部分模型参数。\n",
    "  - 之所以称之为feature extraction是因为把预训练的CNN模型当作一个特征提取模型，利用提取出来的特征完成训练任务。\n",
    "  \n",
    "构建迁移学习的步骤：\n",
    "1. 初始化预训练模型\n",
    "2. 把最后一层的输出层改变成想要分的类别数\n",
    "3. 定义一个optimizer更新参数\n",
    "4. 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51679e8",
   "metadata": {},
   "source": [
    "迁移学习采用的是hymenoptera_data数据集，使用datasets.ImageFolder读取数据。\n",
    "\n",
    "之后用data.DataLoader转成DataLoader，但是在数据读取之前，先做一些参数定义：\n",
    "- 数据路径\n",
    "- 预训练模型的名称\n",
    "- 分类个数\n",
    "- batch_size\n",
    "- 迁移学习的方式\n",
    "- 输入图片的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46d34e",
   "metadata": {},
   "source": [
    "## 初始化预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a5699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to the ImageFolder structure\n",
    "data_dir = \"data/hymenoptera_data\"\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "# when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c427687",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "\n",
    "做一些预处理，同时分成训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc3aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\"train\", \"val\"]}\n",
    "\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "        batch_size=batch_size, shuffle=True, num_workers=4) for x in [\"train\", \"val\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e2002",
   "metadata": {},
   "source": [
    "## 模型初始化操作\n",
    "\n",
    "模型选用resnet模型，同时想用这个网络作特征提取器，那么就需要冻结网络的前面层（即将所有的param.requires_grad=False），不让这些层进行训练，而改了后面的全连接层，把输出分成2分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289e3b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def set_parameter_requires_grad(model, feature_extract):\n",
    "    if feature_extract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    if model_name == 'resnet':\n",
    "        # use_pretrained为True，拿到是torchvision训练好的模型，否则是随机初始化参数\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        # 如果选择后面的这种特征提取模式，会冻结前面的所有参数\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # 拿到resnet18最后的一个fc的in_features, 把之前的1000分类改成2分类\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        # 把最后的一个fc改了\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    else:\n",
    "        print('model not implemented')\n",
    "        return None, None\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, \n",
    "                                        feature_extract, use_pretrained=True)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa1ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 只需要改全连接层的参数，前面的不动\n",
    "print(model_ft.layer1[0].conv1.weight.requires_grad)\n",
    "print(model_ft.fc.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc93aec",
   "metadata": {},
   "source": [
    "同时需要微调前面的参数，此时经常会用到两组学习率进行微调：\n",
    "- 前面的层数用较小的学习率\n",
    "- 后面的层数用较大的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857496ce",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cc8b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, loss_fn, optimizer, num_epochs=5):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.\n",
    "    val_acc_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0.\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                with torch.autograd.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(inputs)    # bsize * 2\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('Phase {} loss: {}, acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d292a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase train loss: 0.682989239692688, acc: 0.5819672131147541\n",
      "Phase val loss: 0.5739691514594882, acc: 0.7254901960784313\n",
      "Phase train loss: 0.5177661625088238, acc: 0.7991803278688525\n",
      "Phase val loss: 0.41445744018149533, acc: 0.8431372549019608\n",
      "Phase train loss: 0.39893033172263476, acc: 0.8647540983606558\n",
      "Phase val loss: 0.32545705363641375, acc: 0.9150326797385621\n",
      "Phase train loss: 0.3505223857574776, acc: 0.8565573770491803\n",
      "Phase val loss: 0.2750035684093151, acc: 0.934640522875817\n",
      "Phase train loss: 0.30311426935625857, acc: 0.8811475409836066\n",
      "Phase val loss: 0.24724241623691484, acc: 0.9281045751633987\n",
      "Phase train loss: 0.2744458361727292, acc: 0.9180327868852459\n",
      "Phase val loss: 0.2307880542247124, acc: 0.934640522875817\n",
      "Phase train loss: 0.2977292711128954, acc: 0.8729508196721312\n",
      "Phase val loss: 0.22461512899087147, acc: 0.934640522875817\n",
      "Phase train loss: 0.25576169080421574, acc: 0.889344262295082\n",
      "Phase val loss: 0.22232871040020113, acc: 0.9281045751633987\n",
      "Phase train loss: 0.23767427489405774, acc: 0.9098360655737705\n",
      "Phase val loss: 0.21269803986050725, acc: 0.9477124183006536\n",
      "Phase train loss: 0.20425877854472302, acc: 0.9180327868852459\n",
      "Phase val loss: 0.22089797560296026, acc: 0.9281045751633987\n",
      "Phase train loss: 0.20619423296607908, acc: 0.930327868852459\n",
      "Phase val loss: 0.20560196665377398, acc: 0.934640522875817\n",
      "Phase train loss: 0.2074607920939805, acc: 0.9426229508196722\n",
      "Phase val loss: 0.20260541477039748, acc: 0.934640522875817\n",
      "Phase train loss: 0.2259450662331503, acc: 0.9016393442622951\n",
      "Phase val loss: 0.19813069447972417, acc: 0.934640522875817\n",
      "Phase train loss: 0.19713680568288583, acc: 0.9426229508196722\n",
      "Phase val loss: 0.20006309693155724, acc: 0.934640522875817\n",
      "Phase train loss: 0.14537906646728516, acc: 0.9672131147540983\n",
      "Phase val loss: 0.1902661153109245, acc: 0.9477124183006536\n"
     ]
    }
   ],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.001, momentum=0.9)   # 这里是筛选出需要更新的参数进行更新\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "_, ohist = train_model(model_ft, dataloaders_dict, loss_fn, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284c666",
   "metadata": {},
   "source": [
    "## 绘制损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9814e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12e0076b9a0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,num_epochs+1),ohist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
