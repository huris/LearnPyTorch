{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa163294",
   "metadata": {},
   "source": [
    "# 准备数据集\n",
    "\n",
    "nltk分词, jieba分词，构建词典，数字编码，形成batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac6e06",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1120f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "# 注意这里需要下载一个punkt包\n",
    "# nltk.download()\n",
    "\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54650da",
   "metadata": {},
   "source": [
    "## 导入数据并分词\n",
    "\n",
    "先导入数据：\n",
    "- train.txt和dev.txt\n",
    "- 每个文件多行，每一行都是英文句子，后面跟着中文翻译，两者用'\\t'分开\n",
    "\n",
    "下面的代码导入数据，已经进行了分词处理，每个句子前后都加了两个特殊符号，和原来给的不一样的地方是尝试了jieba分词，而不是单个汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad50382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Huris\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.465 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')      # 每一行是英文+翻译的形式\n",
    "            #print(line)   # ['Anyone can do that.', '任何人都可以做到。']\n",
    "            #print(nltk.word_tokenize(line[0].lower()))    # ['anyone', 'can', 'do', 'that', '.']\n",
    "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
    "            #print([c for c in line[1]])   ['任', '何', '人', '都', '可', '以', '做', '到', '。']\n",
    "            #print(list(jieba.cut(line[1])))        ['任何人', '都', '可以', '做到', '。']\n",
    "            #cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
    "            cn.append(['BOS'] + list(jieba.cut(line[1])) + ['EOS'])\n",
    "    return en, cn\n",
    "\n",
    "train_file = 'data/nmt/en-cn/train.txt'\n",
    "dev_file = 'data/nmt/en-cn/dev.txt'\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f6e2b",
   "metadata": {},
   "source": [
    "看一下长什么样，每句话分成了一个个单词，接下来会把这些单词对应到词典中的位置上去，用词典中的位置表示这些句子，因为计算机只认识数字，不认识这些汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6f0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'], ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS'], ['BOS', 'she', 'married', 'him', '.', 'EOS']] [['BOS', '任何人', '都', '可以', '做到', '。', 'EOS'], ['BOS', '要', '不要', '再來', '一塊', '蛋糕', '？', 'EOS'], ['BOS', '她', '嫁给', '了', '他', '。', 'EOS']]\n",
      "[['BOS', 'she', 'put', 'the', 'magazine', 'on', 'the', 'table', '.', 'EOS'], ['BOS', 'hey', ',', 'what', 'are', 'you', 'doing', 'here', '?', 'EOS'], ['BOS', 'please', 'keep', 'this', 'secret', '.', 'EOS']] [['BOS', '她', '把', '雜誌', '放在', '桌上', '。', 'EOS'], ['BOS', '嘿', '，', '你', '在', '這做', '什麼', '？', 'EOS'], ['BOS', '請', '保守', '這個', '秘密', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_en[:3], train_cn[:3])\n",
    "print(dev_en[:3], dev_cn[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0757db",
   "metadata": {},
   "source": [
    "## 构建单词表\n",
    "\n",
    "遍历每个句子，统计每个单词出现的个数，按照频率由高到低的顺序把单词放到字典中，并且建立一种字典索引到具体词的一种映射关系。\n",
    "\n",
    "有了字典之后，就可以把上面的数据用数字进行编码，即它们在字典中的位置表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468aef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2    # 两个特殊的字符UNK和PAD\n",
    "    word_dict = {w[0]: index + 2 for index, w in enumerate(ls)}   # 字典的前两个位置放特殊字符\n",
    "    word_dict['UNK'] = UNK_IDX \n",
    "    word_dict['PAD'] = PAD_IDX\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "\n",
    "inv_en_dict = {v:k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v:k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1682e59d",
   "metadata": {},
   "source": [
    "## 数字编码\n",
    "\n",
    "遍历每个句子，得到它们在字典中的位置，为了后续输入方便，这里还根据句子的长度从短到长进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a6ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "    # 根据英语句子的长度排序\n",
    "    def len_argsort(seq):   # 这个seq是一个二维矩阵，每一行是一个句子，且都已经用单词在字典中的位置进行了编码\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "    \n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc5089",
   "metadata": {},
   "source": [
    "这时候，train_en，train_cn这些里面的每个句子的单词都用数字表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e79cc96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1318, 126, 3]\n",
      "BOS 告辞 ！ EOS\n",
      "BOS goodbye ! EOS\n"
     ]
    }
   ],
   "source": [
    "print(train_en[1])\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[1]]))\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8934f42",
   "metadata": {},
   "source": [
    "## 构建batch\n",
    "\n",
    "数字编码之后，接下来就可以划分好一个个batch了\n",
    "\n",
    "步骤，一共有三个函数：\n",
    "- 一个函数是根据batch大小返回batch大小的索引，用于数据集中取具体的句子\n",
    "- 由于每个句子的长度不一致，因此还需要一个函数对句子进行填充，根据最长长度进行0填充\n",
    "- 最后一个函数用于生成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5ce8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入训练集大小，batch_size，返回多批连续的batch_size个索引，每个索引代表个样本\n",
    "# 根据这个索引去拿一个个的batch\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches      # 返回多批连着的bath_size个索引  \n",
    "# get_minibatches(len(train_en), 32)\n",
    "\n",
    "# 数据预处理，每个句子都不等长，通过该函数就可以把句子补齐，不够长的在句子后面添0\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]    # 每个句子的长度\n",
    "    n_samples = len(seqs)       # 一共有多少个句子\n",
    "    max_len = np.max(lengths)      # 找出最大的句子长度\n",
    "    \n",
    "    x = np.zeros((n_samples, max_len)).astype('int32') # 按照最大句子长度生成全0矩阵\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "    for idx, seq in enumerate(seqs):   # 把有句子的位置填充进去\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths    # x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)  # 得到batch个索引\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:   # 每批数据的索引\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n",
    "        # 填充成一样的长度，记录句子的真实长度，这个在后面输入网络的时候得用\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)   # 产生训练集\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)   # 产生验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ff39d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 11) (64,) (64, 15) (64,)\n"
     ]
    }
   ],
   "source": [
    "# 看一下batch\n",
    "print(train_data[1][0].shape, train_data[1][1].shape, train_data[1][2].shape, train_data[1][3].shape)    \n",
    "# 第一个维度表示第1个batch\n",
    "# 第二个维度[0]代表英文个数，[1]代表英文长度\n",
    "# [2]代表中文词个数，[3]代表中文长度\n",
    "# 每个batch里的句子长度是不一样的，同一batch里面的句子长度由于填充0使得一样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f65de1",
   "metadata": {},
   "source": [
    "# Seq2Seq模型\n",
    "\n",
    "Seq2Seq由编码器和解码器组成，即两个GRU组成。\n",
    "- 编码器负责句子的编码任务，把句子的含义统一集中在最后一个时间步的隐藏状态中\n",
    "- 解码器进行翻译\n",
    "\n",
    "下面的工作是先搭建编码器和解码器的GRU，然后再把两者结合起来组成语言模型。\n",
    "<img style=\"float: center;\" src=\"images/15.png\" width=\"70%\">\n",
    "\n",
    "两者都用RNN组成，编码器接收输入的句子，然后进行编码计算，把句子的所有信息综合到最后的一个时间步隐藏状态上，然后作为解码器的初始隐藏状态，开始解码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4da19",
   "metadata": {},
   "source": [
    "## 编码器\n",
    "\n",
    "编码器接收一个batch（64个句子），每个句子都在它们各自在字典中的位置进行表示，所以输入就是一个[batch_size, seq_len]，之后会经过一个embedding层进行每个单词的词嵌入表示，此时数据维度就变成了[batch_size, seq_len, embed_size]，这个与之前的思路一致，但是有一点不同，就是在embedding之前，需要将句子长度从长到短进行排序，因为这次输入的是序列。\n",
    "<img style=\"float: center;\" src=\"images/16.png\" width=\"70%\">\n",
    "\n",
    "虽然形成batch的时候填充0使它们维度一致，但这些无效的0字符在输入GRU时候需要pack掉，因为padding多余的0之后，会导致GRU对它的表示通过了非常多无用的字符，这样得到的句子表示会有误差。\n",
    "\n",
    "如果想要进行后续的pack操作，需要先对句子进行长到短的排序，这时候经过embedding就得到嵌入之后的数据。之后就是想办法处理这些变长的序列。\n",
    "\n",
    "在处理之前，先感觉一下变长序列的处理效果，例如yes这句话，如果不加处理，是这样输入到GRU的：\n",
    "<img style=\"float: center;\" src=\"images/17.png\" width=\"70%\">\n",
    "\n",
    "而处理之后，是这样的：\n",
    "<img style=\"float: center;\" src=\"images/18.png\" width=\"70%\">\n",
    "\n",
    "PyTorch给出了处理这两种情况的两个函数：\n",
    "- nn.utils.rnn.pack_padded_sequence()\n",
    "  - 压缩已经经过填充的序列\n",
    "  - 因为之前把变长序列经过填充变成等长，需要先压缩一下\n",
    "- nn.utils.rnn.pad_packed_sequence()\n",
    "  - 填充已经被压缩的序列\n",
    "  - 需要通过这个函数再变成压缩之前的样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68e17ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [2.],\n",
       "         [3.]],\n",
       "\n",
       "        [[5.],\n",
       "         [0.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "tensor_in = torch.FloatTensor([[1, 2, 3], [5, 0, 0]]).resize_(2, 3, 1)\n",
    "tensor_in = Variable(tensor_in)\n",
    "seq_lenghs = [3, 1]\n",
    "tensor_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb0292",
   "metadata": {},
   "source": [
    "此时如果对其排序（目前已经从长到短排好序了），而真实的数据是从短到长排序，因此需要经历这样的一句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8b15e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([3, 1]),\n",
       "indices=tensor([0, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(seq_lenghs).sort(0, descending=True)    \n",
    "# 返回两个值，第一个是排好序的数组，第二个是每个元素在原数组里面的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56e5c3",
   "metadata": {},
   "source": [
    "把填充的数据进行pack，可以发现第二个样本的0都不见了，只留了关键的那些单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd119db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1.],\n",
       "        [5.],\n",
       "        [2.],\n",
       "        [3.]]), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack = nn.utils.rnn.pack_padded_sequence(tensor_in, seq_lenghs, batch_first=True)\n",
    "pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cbb51",
   "metadata": {},
   "source": [
    "此时如果经过一个RNN（三层的RNN），可以顺便看一下输出h长什么样，因为这个例子基本上完全模拟PyTorch处理变长训练的关键之处，也是解码器的核心部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed827fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2]),\n",
       " PackedSequence(data=tensor([[-0.5723, -0.3532],\n",
       "         [ 0.3066,  0.8889],\n",
       "         [-0.6507, -0.0175],\n",
       "         [-0.5896, -0.1284]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None),\n",
       " tensor([[[ 0.3008, -0.2164],\n",
       "          [ 0.5393, -0.4938]],\n",
       " \n",
       "         [[ 0.8907,  0.8570],\n",
       "          [-0.4533, -0.5563]],\n",
       " \n",
       "         [[-0.5896, -0.1284],\n",
       "          [ 0.3066,  0.8889]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(1, 2, 3, batch_first=True)   # 输入维度是1(embed_dim)，输出维度是2(2个隐藏单元), 3层\n",
    "h0 = Variable(torch.randn(3, 2, 2))  # h0的初始状态， (layers_num*direction_nums, batch_size, hidden_size)\n",
    "\n",
    "out, h = rnn(pack, h0)\n",
    "out[0].shape, out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39fdf5",
   "metadata": {},
   "source": [
    "看h（最后一层的最后一个时间步的隐藏状态），解码器就是以这个作为新的GRU初始隐藏状态的输入，想要获取这个就是h[[-1]]。\n",
    "<img style=\"float: center;\" src=\"images/19.png\" width=\"70%\">\n",
    "\n",
    "得到上面out输出之后，接下来就是pad_packed_sequence()\n",
    "\n",
    "经过这一轮的操作，变成序列合理的处理，先压缩，然后再填充回去就得到和原来一致的维度关系，这也是PyTorch处理变成序列的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb734888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5723, -0.3532],\n",
       "          [-0.6507, -0.0175],\n",
       "          [-0.5896, -0.1284]],\n",
       " \n",
       "         [[ 0.3066,  0.8889],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]]], grad_fn=<TransposeBackward0>),\n",
       " tensor([3, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpackded = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "unpackded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b51daa",
   "metadata": {},
   "source": [
    "编码器中，经过这样的操作之后，就可以使得变长序列准确地通过GRU得到每个单词地表示，然后再换回原来的顺序（句子短到长），然后拿到最后地输出和最后一个隐藏状态即可，这就是编码器的前向传播过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11fd92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        # 第一个维度应该是embed_size，这里为了方便，相等了\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):  \n",
    "        # 输入lengths，每个句子是不等长，需要每个句子最后一个时间步的隐藏状态,\n",
    "        # 因此所以需要知道句子长度，x表示一个batch里面的句子 \n",
    "        \n",
    "        # 把batch里面的seq按照长度排序\n",
    "        # sorted_len表示排好序的数组，sorted_index表示每个元素在原数组位置\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]  # 句子已经按照seq长度排好序\n",
    "        embedded = self.dropout(self.embed(x_sorted))  # [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        # 处理变长序列\n",
    "        # data.numpy()是原来张量的克隆，转成numpy数组，相当于clone().numpy()\n",
    "        # 把变长序列的0都给去掉，之前填充的字符都给压扁\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        # 得到batch中每个样本的真实隐藏状态\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        # 填充回去\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        # 让短的句子在前面\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        # contiguous是为了把不连续的内存单元连续起来 \n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        return out, hid[[-1]]   # 把最后一层的hid给拿出来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab3785",
   "metadata": {},
   "source": [
    "## PyTorch中的GRU\n",
    "\n",
    "torch.nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "- input_size：（seq_len, batch, input_size），这个PyTorch默认是seq_len是第一维，如果不设置batch_first维True的时候，就需要把输入转成这样的维度才行。input_size是特征数量。\n",
    "- h_0：（num_layers\\*num_directions, batch, hidden_size），如果不提供的话，默认初始化为0\n",
    "\n",
    "再看一下这个层的输出：\n",
    "- output：(seq_len, batch, num_directions\\*hidden_size)，output是**最后一层所有时间步的隐藏状态**，seq_len代表所有时间步，batch代表每个样本，num_directions\\*hidden_size就是每个方向上隐藏单元个数。\n",
    "- h_n：(num_layers\\*num_directions, batch, hidden_size)，**最后一个时间步所有曾的隐藏状态**，第一个维度是层数和方向数，第二个是样本，第三个是每一层每个时间步的隐藏单元个数。\n",
    "<img style=\"float: center;\" src=\"images/20.png\" width=\"70%\">\n",
    "\n",
    "因此，可以很容易拿到最后一层某个时间步的隐藏状态和最后一个时间步某个层的隐藏状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91fdc0",
   "metadata": {},
   "source": [
    "## 解码器\n",
    "\n",
    "也是一个GRU，隐藏状态的初始值来自于编码器最后一个时间步的h，而不是随机初始化的一个h。\n",
    "\n",
    "编码器这里接收中文输入，需要基于上一个词才能预测出下一个词。\n",
    "\n",
    "编码器前向传播过程：接受输入是y（中文），大小是[batch_size, seq_len - 1]，这里减一是因为不用全部，基于第一个就能预测第二个，所以基于前n-1个就能预测出全部来。同样也会先embedding，但是embedding之前依然是句子从长到短排序，依然需要压缩还原，与编码器代码几乎一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bae80c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个基本上和Encoder是一致的，无非就是初始化的h换成了Encoder之后的h\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        # y: [batch_size, seq_len-1]\n",
    "        # 句子从长到短排序\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        # [batch_size, outpout_length, embed_size]\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        \n",
    "        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(pack_seq, hid)   # 每个有效时间步单词的最后一层的隐藏状态\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)   # [batch, seq_len-1, hidden_size]\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()  # [batch, seq_len-1, hidden_size]\n",
    "        \n",
    "        hid = hid[:, original_idx.long()].contiguous()   # [1, batch, hidden_size]\n",
    "        output = F.log_softmax(self.out(output_seq), -1)        \n",
    "        # [batch, seq_len-1, vocab_size]   每个样本每个时间步长都有一个vocab_size的维度长度，表示每个单词的概率\n",
    "        \n",
    "        return output, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160e1df",
   "metadata": {},
   "source": [
    "## Seq2Seq模型\n",
    "\n",
    "把编码器和解码器连起来组成一个简单的Seq2Seq模型。\n",
    "\n",
    "前向传播就是编码器对英文句子编码计算，得到最后一个时间步的h，这个可以理解蕴涵着输入句子的信息，然后到解码器中解码，得到输出output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c672634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder   \n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)  # encoder 进行编码\n",
    "        output, hid = self.decoder(y, y_lengths, hid)  # deocder 负责解码\n",
    "        return output, None\n",
    "\n",
    "    # 对句子进行翻译，max_length句子的最大长度\n",
    "    # 测试最后模型效果的时候用，与Seq2Seq本身没有啥关系\n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)   # 解码\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):   \n",
    "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "        \n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6b0e6",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "需要写一个损失函数，因为句子是变长的，需要用mask处理那些无用的填充字符，类似于写一个NLLoss的东西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3bce561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, input, target, mask):\n",
    "        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性\n",
    "        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size]\n",
    "        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1]\n",
    "        \n",
    "        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
    "        # 在每个vocab_size维度取正确单词的索引，里面有很多是填充的，mask去掉这些填充\n",
    "        output = -input.gather(1, target) * mask\n",
    "        # 这个其实在写一个NLloss，也就是sortmax的取负号\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        \n",
    "        return output  # [batch_size*seq_len-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58eb9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f74e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 9.35302448272705\n",
      "Epoch 0 iteration 100 loss 6.020293235778809\n",
      "Epoch 0 iteration 200 loss 4.854287147521973\n",
      "Epoch 0 Training loss 5.889250806130928\n",
      "Evaluation loss 5.210167619484115\n",
      "Epoch 1 iteration 0 loss 4.4433417320251465\n",
      "Epoch 1 iteration 100 loss 5.562403202056885\n",
      "Epoch 1 iteration 200 loss 4.349039554595947\n",
      "Epoch 1 Training loss 4.958273569633383\n",
      "Epoch 2 iteration 0 loss 3.9863054752349854\n",
      "Epoch 2 iteration 100 loss 5.250918388366699\n",
      "Epoch 2 iteration 200 loss 4.048994064331055\n",
      "Epoch 2 Training loss 4.62348482656324\n",
      "Epoch 3 iteration 0 loss 3.6867971420288086\n",
      "Epoch 3 iteration 100 loss 5.00648832321167\n",
      "Epoch 3 iteration 200 loss 3.8102409839630127\n",
      "Epoch 3 Training loss 4.375772453130721\n",
      "Epoch 4 iteration 0 loss 3.45863676071167\n",
      "Epoch 4 iteration 100 loss 4.806687831878662\n",
      "Epoch 4 iteration 200 loss 3.646423578262329\n",
      "Epoch 4 Training loss 4.166808362241544\n",
      "Epoch 5 iteration 0 loss 3.2544803619384766\n",
      "Epoch 5 iteration 100 loss 4.627119064331055\n",
      "Epoch 5 iteration 200 loss 3.4618587493896484\n",
      "Epoch 5 Training loss 3.9848584923218984\n",
      "Evaluation loss 4.461812660677348\n",
      "Epoch 6 iteration 0 loss 3.0630362033843994\n",
      "Epoch 6 iteration 100 loss 4.478766441345215\n",
      "Epoch 6 iteration 200 loss 3.295151472091675\n",
      "Epoch 6 Training loss 3.818979859434203\n",
      "Epoch 7 iteration 0 loss 2.8973889350891113\n",
      "Epoch 7 iteration 100 loss 4.346400260925293\n",
      "Epoch 7 iteration 200 loss 3.1394476890563965\n",
      "Epoch 7 Training loss 3.667947690258399\n",
      "Epoch 8 iteration 0 loss 2.751037120819092\n",
      "Epoch 8 iteration 100 loss 4.212331771850586\n",
      "Epoch 8 iteration 200 loss 3.0258617401123047\n",
      "Epoch 8 Training loss 3.5260633227893954\n",
      "Epoch 9 iteration 0 loss 2.6278789043426514\n",
      "Epoch 9 iteration 100 loss 4.110342025756836\n",
      "Epoch 9 iteration 200 loss 2.924231767654419\n",
      "Epoch 9 Training loss 3.3959762394990265\n",
      "Epoch 10 iteration 0 loss 2.5417392253875732\n",
      "Epoch 10 iteration 100 loss 4.01880407333374\n",
      "Epoch 10 iteration 200 loss 2.770611047744751\n",
      "Epoch 10 Training loss 3.2760407940375753\n",
      "Evaluation loss 4.312050665715179\n",
      "Epoch 11 iteration 0 loss 2.360609292984009\n",
      "Epoch 11 iteration 100 loss 3.882397413253784\n",
      "Epoch 11 iteration 200 loss 2.6779232025146484\n",
      "Epoch 11 Training loss 3.1595201449074786\n",
      "Epoch 12 iteration 0 loss 2.2866299152374268\n",
      "Epoch 12 iteration 100 loss 3.7849295139312744\n",
      "Epoch 12 iteration 200 loss 2.5601155757904053\n",
      "Epoch 12 Training loss 3.0528498488641262\n",
      "Epoch 13 iteration 0 loss 2.180013418197632\n",
      "Epoch 13 iteration 100 loss 3.6942412853240967\n",
      "Epoch 13 iteration 200 loss 2.4727213382720947\n",
      "Epoch 13 Training loss 2.951858093471696\n",
      "Epoch 14 iteration 0 loss 2.103147268295288\n",
      "Epoch 14 iteration 100 loss 3.6239664554595947\n",
      "Epoch 14 iteration 200 loss 2.347912311553955\n",
      "Epoch 14 Training loss 2.855558746133723\n",
      "Epoch 15 iteration 0 loss 2.034365653991699\n",
      "Epoch 15 iteration 100 loss 3.534237861633301\n",
      "Epoch 15 iteration 200 loss 2.3173296451568604\n",
      "Epoch 15 Training loss 2.7646939493235583\n",
      "Evaluation loss 4.285326453467082\n",
      "Epoch 16 iteration 0 loss 1.9489964246749878\n",
      "Epoch 16 iteration 100 loss 3.473796844482422\n",
      "Epoch 16 iteration 200 loss 2.2339611053466797\n",
      "Epoch 16 Training loss 2.6799757955004173\n",
      "Epoch 17 iteration 0 loss 1.8778492212295532\n",
      "Epoch 17 iteration 100 loss 3.393612861633301\n",
      "Epoch 17 iteration 200 loss 2.16239333152771\n",
      "Epoch 17 Training loss 2.5998395962270506\n",
      "Epoch 18 iteration 0 loss 1.8492964506149292\n",
      "Epoch 18 iteration 100 loss 3.3367369174957275\n",
      "Epoch 18 iteration 200 loss 2.09023118019104\n",
      "Epoch 18 Training loss 2.5243337233817082\n",
      "Epoch 19 iteration 0 loss 1.8032617568969727\n",
      "Epoch 19 iteration 100 loss 3.250532627105713\n",
      "Epoch 19 iteration 200 loss 2.032461643218994\n",
      "Epoch 19 Training loss 2.453150281264721\n"
     ]
    }
   ],
   "source": [
    "# 定义训练和验证函数\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # 解码器那边的输入， 输入一个单词去预测另外一个单词\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 因为没有了最后一个  [batch_size, seq_len-1]\n",
    "            mb_y_len[mb_y_len<=0] =  1   # 这句话是为了以防出错\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]  \n",
    "            # [batch_size, mb_y_len.max()], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n",
    "            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print('Evaluation loss', total_loss / total_num_words)\n",
    "\n",
    "\n",
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            # 更新\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 这里防止梯度爆炸， 这是和以往不太一样的地方\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n",
    "            \n",
    "        print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)\n",
    "        \n",
    "# 训练\n",
    "train(model, train_data, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b9515",
   "metadata": {},
   "source": [
    "mask制作方式：遮盖掉那些填充的那部分字符，这时候计算损失时，就只计算有效部分。\n",
    "\n",
    "假设有5个句子，长度分别是：3，8，10，2，1，可以先根据最大长度10生成0-10的一个数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "708591d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([3, 8, 10, 2, 1])\n",
    "torch.arange(m.max().item())[None, :]   # tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec99297",
   "metadata": {},
   "source": [
    "这个就是上面的：\n",
    "- mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "- [None, :]操作扩展第一维度，类似[np.newaxis, :]\n",
    "- 后半部分：m[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58586312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 8],\n",
       "        [10],\n",
       "        [ 2],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207530e",
   "metadata": {},
   "source": [
    "此时，如果两边执行小于操作，就会得到如下结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6967b473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(m.max().item())[None, :] < m[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a167e",
   "metadata": {},
   "source": [
    "此时，每一行代表一个句子，而True部分代表句子真实长度，False代表填充部分，这时候再去取预测单词的具体位置，就可以把无用的字符过滤掉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438c2ce",
   "metadata": {},
   "source": [
    "## 结果预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ecda82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])  #原来的英文\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])  #原来的中文\n",
    "    print(\"\".join(cn_sent))\n",
    " \n",
    "    # 一条句子\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)  # shape:[1,1], [[2]]\n",
    "    \n",
    "    # y_lengths: [[2]], 一个句子\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)  # [1, 10]\n",
    "    # 映射成中文\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))           #翻译后的中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2e0d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮膚 真好 。 EOS\n",
      "你看上去。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 UNK 正确 。 EOS\n",
      "你是你的。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每個 人 都 佩服 他 的 勇氣 。 EOS\n",
      "每個人都認識他。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几点 了 ？ EOS\n",
      "它是什么？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今晚 有空 。 EOS\n",
      "我很忙。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這是 你 的 書 。 EOS\n",
      "你是个美人。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他们 在 吃 午饭 。 EOS\n",
      "他们是个好人。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這把 椅子 UNK 。 EOS\n",
      "这是一个生和死的。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 UNK 。 EOS\n",
      "它是什么。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很多 人 都 参加 了 他 的 UNK 。 EOS\n",
      "他的意見有道理。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训练 。 EOS\n",
      "保持安静。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有人 在 看 著 你 。 EOS\n",
      "汤姆在做。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我的房子。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜歡 流行 音樂 。 EOS\n",
      "我喜歡爵士樂。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS Tom 沒有 孩子 。 EOS\n",
      "汤姆在波士顿。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 UNK 上 。 EOS\n",
      "請說話。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤姆 冷静下来 了 。 EOS\n",
      "汤姆挺受欢迎。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大聲 一點兒 。 EOS\n",
      "请不要出門。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周日 空 出来 。 EOS\n",
      "繼續下週開學。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS UNK 了 一個 錯 。 EOS\n",
      "我有個提案。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d61cc",
   "metadata": {},
   "source": [
    "# Attention模型\n",
    "\n",
    "Luong的Attention模型：\n",
    "- 论文：《Effective Approaches to Attention-based Neural Machine Translation》\n",
    "- https://arxiv.org/pdf/1508.04025.pdf\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/21.png\" width=\"70%\">\n",
    "\n",
    "Attention与之前模型的不同之处在于这里有一个$c_t$，即上下文向量。\n",
    "\n",
    "之前的简单结构，是编码器编码之后拿到最后一个隐藏状态直接给解码器，解码器拿到h，拿到前一步的$y_{t-1}$，就可以进行$y_t$的翻译，但是这里不一样。\n",
    "\n",
    "解码器的最后输出部分，是经过一个Attention机制，预测当前输出的时候都会给所有输入加一个权重来表示输入与当前输出的一个关联关系，权重越大，说明当前输入对当前输出的重要性越大。\n",
    "\n",
    "这个权重，根据论文这样得到：\n",
    "<img style=\"float: center;\" src=\"images/22.png\" width=\"70%\">\n",
    "\n",
    "这里的score，有三种方式，这里选用第一种直接点积，表示二者相似程度。\n",
    "<img style=\"float: center;\" src=\"images/23.png\" width=\"70%\">\n",
    "\n",
    "这里的重点是Attention机制，儿Attention机制就是在求一个权重，权重乘以输入就得到一个上下文向量，这个上下文向量又关注了蕴涵的输入信息。上下文向量和当前输出的隐藏状态合起来再经过tanh，再经过一个线性层就得到了最终的输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda95280",
   "metadata": {},
   "source": [
    "## 编码器\n",
    "\n",
    "编码器部分与之前一致，任何是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的上下文向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "478389ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size*2, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))   # [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # [batch_size, seq_len, 2*enc_hidden_size]\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()   # [batch_size, seq_len, 2*enc_hidden_size]\n",
    "        hid = hid[:, original_idx.long()].contiguous()   # [2, batch_size, enc_hidden_size]\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)   # 双向的GRU，这里是最后一个状态， 联结起来  [batch_size, 2*enc_hidden_size]\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)  # [1, batch_size, dec_hidden_size]\n",
    "        \n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73b844",
   "metadata": {},
   "source": [
    "## Attention机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50b61e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        \n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2+dec_hidden_size, dec_hidden_size)\n",
    "    \n",
    "    def forward(self, output, encoder_output, mask):\n",
    "        # output: [batch_size, seq_len_y-1, dec_hidden_size]  这个output 是decoder的每个时间步输出的隐藏状态\n",
    "        # encoder_output: [batch_size, seq_len_x, 2*enc_hidden_size]\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = encoder_output.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(encoder_output.view(batch_size*input_len, -1))  # [batch_size*seq_len_x,dec_hidden_size]\n",
    "        context_in = context_in.view(batch_size, input_len, -1)  # [batch_size, seq_len_x, dec_hidden_size]\n",
    "        context_in = context_in.transpose(1, 2)   # [batch_size, dec_hidden_size, seq_len_x]\n",
    "        \n",
    "        attn = torch.bmm(output, context_in)  # [batch_size, seq_len_y-1, seq_len_x]\n",
    "        # 这个东西就是求得当前时间步的输出output和所有输入相似性关系的一个得分score , 下面就是通过softmax把这个得分转成权重\n",
    "        attn = F.softmax(attn, dim=2)    # 此时第二维度的数字全都变成了0-1之间的数， 越大表示当前的输出output与哪个相关程度越大\n",
    "        \n",
    "        context = torch.bmm(attn, encoder_output)   # [batch_size, seq_len_y-1, 2*enc_hidden_size]\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2)  # [batch_size, seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
    "        \n",
    "        output = output.view(batch_size*output_len, -1)   # [batch_size*seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
    "        output = torch.tanh(self.linear_out(output))     # [batch_size*seq_len_y-1, dec_hidden_size]\n",
    "        output = output.view(batch_size, output_len, -1)  # [batch_size, seq_len_y-1, dec_hidden_size]\n",
    "        \n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ec5ee",
   "metadata": {},
   "source": [
    "## 解码器\n",
    "\n",
    "decoder会根据已经翻译好的句子内容和当前的context vector，来决定下一个输出的单词，这里与之前不同的之处在于最后输出经过了一个Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e0dda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len*y_len\n",
    "        x_mask = torch.arange(x_len.max(), device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(y_len.max(), device=x_len.device)[None, :] < y_len[:, None]\n",
    "        \n",
    "        x_mask = x_mask.float()\n",
    "        y_mask = y_mask.float()\n",
    "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, encoder_out, encoder_out_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]   # 句子从长到短排序\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted))     # [batch_size, output_length, embed_size]\n",
    "        \n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()   # [batch_size, seq_len_y-1, dec_hidden_size]\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        mask = self.create_mask(y_lengths, encoder_out_lengths)\n",
    "        \n",
    "        output, attn = self.attention(output_seq, encoder_out, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02b981",
   "metadata": {},
   "source": [
    "## Seq2Seq+Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ab8ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(encoder_out, x_lengths, y, y_lengths, hid)\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out, x_lengths, y, torch.ones(batch_size).long().to(y.device), hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        \n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9d501",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "170ce19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "embed_size = 20\n",
    "encoder = Encoder(vocab_size=en_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29d12199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 9.34937572479248\n",
      "Epoch 0 iteration 100 loss 6.319538116455078\n",
      "Epoch 0 iteration 200 loss 5.25015115737915\n",
      "Epoch 0 Training loss 6.127085370002386\n",
      "Evaluation loss 5.603679679314572\n",
      "Epoch 1 iteration 0 loss 4.906866550445557\n",
      "Epoch 1 iteration 100 loss 5.99578857421875\n",
      "Epoch 1 iteration 200 loss 4.916123390197754\n",
      "Epoch 1 Training loss 5.429204816858988\n",
      "Epoch 2 iteration 0 loss 4.606022834777832\n",
      "Epoch 2 iteration 100 loss 5.843180179595947\n",
      "Epoch 2 iteration 200 loss 4.719799041748047\n",
      "Epoch 2 Training loss 5.2283853432862495\n",
      "Epoch 3 iteration 0 loss 4.401919841766357\n",
      "Epoch 3 iteration 100 loss 5.7497100830078125\n",
      "Epoch 3 iteration 200 loss 4.547033786773682\n",
      "Epoch 3 Training loss 5.082325355668237\n",
      "Epoch 4 iteration 0 loss 4.234122276306152\n",
      "Epoch 4 iteration 100 loss 5.561656951904297\n",
      "Epoch 4 iteration 200 loss 4.357476234436035\n",
      "Epoch 4 Training loss 4.909747890392661\n",
      "Epoch 5 iteration 0 loss 4.047961235046387\n",
      "Epoch 5 iteration 100 loss 5.4158711433410645\n",
      "Epoch 5 iteration 200 loss 4.194159030914307\n",
      "Epoch 5 Training loss 4.738963226517298\n",
      "Evaluation loss 4.949420515251146\n",
      "Epoch 6 iteration 0 loss 3.830834150314331\n",
      "Epoch 6 iteration 100 loss 5.264143466949463\n",
      "Epoch 6 iteration 200 loss 4.048060417175293\n",
      "Epoch 6 Training loss 4.584298669542553\n",
      "Epoch 7 iteration 0 loss 3.695286273956299\n",
      "Epoch 7 iteration 100 loss 5.129791736602783\n",
      "Epoch 7 iteration 200 loss 3.916126251220703\n",
      "Epoch 7 Training loss 4.451197809438338\n",
      "Epoch 8 iteration 0 loss 3.5294480323791504\n",
      "Epoch 8 iteration 100 loss 5.019106388092041\n",
      "Epoch 8 iteration 200 loss 3.817579507827759\n",
      "Epoch 8 Training loss 4.321117837638611\n",
      "Epoch 9 iteration 0 loss 3.4384641647338867\n",
      "Epoch 9 iteration 100 loss 4.905967712402344\n",
      "Epoch 9 iteration 200 loss 3.710846185684204\n",
      "Epoch 9 Training loss 4.202557841160036\n",
      "Epoch 10 iteration 0 loss 3.3039658069610596\n",
      "Epoch 10 iteration 100 loss 4.8172383308410645\n",
      "Epoch 10 iteration 200 loss 3.565887451171875\n",
      "Epoch 10 Training loss 4.091292919304999\n",
      "Evaluation loss 4.64747279880313\n",
      "Epoch 11 iteration 0 loss 3.1898293495178223\n",
      "Epoch 11 iteration 100 loss 4.707930564880371\n",
      "Epoch 11 iteration 200 loss 3.4714784622192383\n",
      "Epoch 11 Training loss 3.985500948762188\n",
      "Epoch 12 iteration 0 loss 3.0802080631256104\n",
      "Epoch 12 iteration 100 loss 4.6266255378723145\n",
      "Epoch 12 iteration 200 loss 3.3725545406341553\n",
      "Epoch 12 Training loss 3.8831281576422847\n",
      "Epoch 13 iteration 0 loss 2.9708797931671143\n",
      "Epoch 13 iteration 100 loss 4.533605575561523\n",
      "Epoch 13 iteration 200 loss 3.2950289249420166\n",
      "Epoch 13 Training loss 3.7868960310469224\n",
      "Epoch 14 iteration 0 loss 2.9354474544525146\n",
      "Epoch 14 iteration 100 loss 4.45009183883667\n",
      "Epoch 14 iteration 200 loss 3.165102005004883\n",
      "Epoch 14 Training loss 3.69565105560738\n",
      "Epoch 15 iteration 0 loss 2.82944655418396\n",
      "Epoch 15 iteration 100 loss 4.379319190979004\n",
      "Epoch 15 iteration 200 loss 3.122680425643921\n",
      "Epoch 15 Training loss 3.6082331804386887\n",
      "Evaluation loss 4.526919109939419\n",
      "Epoch 16 iteration 0 loss 2.742070198059082\n",
      "Epoch 16 iteration 100 loss 4.296763896942139\n",
      "Epoch 16 iteration 200 loss 3.042555570602417\n",
      "Epoch 16 Training loss 3.5266549366466085\n",
      "Epoch 17 iteration 0 loss 2.6954286098480225\n",
      "Epoch 17 iteration 100 loss 4.209632873535156\n",
      "Epoch 17 iteration 200 loss 2.9487242698669434\n",
      "Epoch 17 Training loss 3.4440580284893967\n",
      "Epoch 18 iteration 0 loss 2.5975329875946045\n",
      "Epoch 18 iteration 100 loss 4.130420684814453\n",
      "Epoch 18 iteration 200 loss 2.908620834350586\n",
      "Epoch 18 Training loss 3.370380488786042\n",
      "Epoch 19 iteration 0 loss 2.4987313747406006\n",
      "Epoch 19 iteration 100 loss 4.056612491607666\n",
      "Epoch 19 iteration 200 loss 2.837895154953003\n",
      "Epoch 19 Training loss 3.2977833462859394\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96234536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮膚 真好 。 EOS\n",
      "你有狗。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 UNK 正确 。 EOS\n",
      "你是个很好。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每個 人 都 佩服 他 的 勇氣 。 EOS\n",
      "他的父母都知道了。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几点 了 ？ EOS\n",
      "那是什么？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今晚 有空 。 EOS\n",
      "我很喜欢。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這是 你 的 書 。 EOS\n",
      "你的狗是你的。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他们 在 吃 午饭 。 EOS\n",
      "他們有這裡。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這把 椅子 UNK 。 EOS\n",
      "这是一個好的。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 UNK 。 EOS\n",
      "它是危險的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很多 人 都 参加 了 他 的 UNK 。 EOS\n",
      "他在狗上了。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训练 。 EOS\n",
      "讓我們的时候。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有人 在 看 著 你 。 EOS\n",
      "你的狗是你的。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我的祖父他。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜歡 流行 音樂 。 EOS\n",
      "我不喜欢。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS Tom 沒有 孩子 。 EOS\n",
      "汤姆在笑。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 UNK 上 。 EOS\n",
      "请清洗。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤姆 冷静下来 了 。 EOS\n",
      "汤姆在波士顿。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大聲 一點兒 。 EOS\n",
      "不要应该再。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周日 空 出来 。 EOS\n",
      "这本书了。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS UNK 了 一個 錯 。 EOS\n",
      "我是個好人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "semg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
